[["index.html", "My R and Statistics Book Preface", " My R and Statistics Book Dino Curic 2022-01-19 Preface "],["glossary.html", "1 Glossary 1.1 Shortcuts 1.2 Statistical Symbols", " 1 Glossary 1.1 Shortcuts 1.1.1 RStudio Environment Action Keyboard Show Keyboard Shortcuts Quick Reference alt + shift + k (Un)comment code cmd + shift + c Reflow comment lines ctrl + shift + 7 or / Restarting R session cmd + shift+ f10 Rename several instances cmd + alt + shift + m Tidy up code cmd + shift + a Help for function where cursor currently f1 1.1.2 Navigating and Sections Action Keyboard Insert Code Section (——) ctrl + shift + r Jump to Specific Code Section cmd + alt + shift + j Fold All Sections cmd + alt + o Expand All Sections cmd + alt + shift + o Right/Left tab ctrl + alt + left or right 1.1.3 Code Execution Action Keyboard Run Selected Lines(s) cmd + enter Run Code Section cmd + alt + t Run Code from Beginning to Line cmd + alt + b Run Code from Line to End cmd + alt + e Run all Code cmd + alt + r 1.1.4 Special Characters Character Keyboard Square brackets alt + 5 Tilde ~ symbol alt + n Pipe symbol + shift + m OR alt + 7 and &amp; Modulo/Remainder 1 %% 1 equal sign 1 == 1 not equal sign 1 != 1 use variable names like numbers or with spaces ’1999’ or 'Country Name’ 1.2 Statistical Symbols 1.2.1 Special Symbols Special Symbol Meaning \\(A\\) \\(\\approx\\) \\(B\\) A is approximately equal to B 1.2.2 Sample and Population Symbols Description Sample Statistic Population Parameter Number of sample or population \\(n\\) \\(N\\) Mean \\(\\bar{x}\\) (“x bar or x hat”) \\(\\mu\\) (“mu”) or \\(\\mu_x\\) (“mu x”) Median \\(\\tilde{x}\\) (“x tilde”) none Variance \\(s^2\\) \\(\\sigma^2\\) (“sigma squared”) Standard Deviation \\(s\\) or \\(sd\\) \\(\\sigma\\) (“sigma”) Proportion \\(\\hat{p}\\) (“p hat”) \\(p\\) Coefficient of Linear Regression \\(r\\) \\(\\rho\\) (“rho”) \\(\\mu\\) and \\(\\sigma\\) can take subscripts to show what you are taking the mean or standard deviation of. For instance, \\(\\sigma_{\\bar{X}}\\) (“sigma sub x-bar”) is the standard deviation of sample means, or standard error of the mean. "],["r-playgrounds.html", "2 R Playgrounds 2.1 Expected Value and Standard Deviation 2.2 Theoretical and Experimental Standard Error 2.3 Sample Size Resulting in 0.01 SE 2.4 Bank and Loans 2.5 Real Polling Data", " 2 R Playgrounds 2.1 Expected Value and Standard Deviation Roulette table example with Red, Black and Green. Only Green wins. Whats the probability that you get Green? v = rep(c(&quot;Red&quot;, &quot;Black&quot;, &quot;Green&quot;), c(18,18,2)) prop.table(table(v)) ## v ## Black Green Red ## 0.47368421 0.05263158 0.47368421 This is the sampling model. x = sample(c(17,-1), prob = c(2/38,36/38)) The expected value is calculated by adding the possible value times their likelyhood together. Its formula is ap + b(1-p). The expected value for 1000 draws. EV = 1000 * (17*2/38 + (-1*36/38)) The standard error (standard deviation of random variables i.e., probability distributions). Its formula is a - b * sqrt(p * (1-p)). The standard error for 1000 draws. SE = sqrt(1000) * (-((-1)-17) * sqrt(2/38 * 36/38)) Random variable S storing the experimental values from sampling model. set.seed(1) S = sample(c(17,-1), size = 1000, replace = T, prob = c(2/38, 36/38)) sum(S) ## [1] -10 Create the experimental sampling distribution of the sample sum. roulette_winnings = function(){ S = sample(c(17,-1), size = 1000, replace = T, prob = c(2/38, 36/38)) sum(S) } set.seed(1) S = replicate(10000, roulette_winnings()) hist(S) The mean or expected value of X? mean(S) ## [1] -52.3324 The standard deviation or standard error of X? sd(S) ## [1] 126.9762 The probabilty that we win? mean(S &gt; 0) ## [1] 0.3391 n = 1000 pbinom(500, size = 1000, prob = 1/19) # ?? ## [1] 1 2.2 Theoretical and Experimental Standard Error The sample statistics: 0.45 “democrats and a sample size of 100. p = 0.45 n = 100 The theoretical standard error for p = 0.45 and sample size n = 100. SE = sqrt(p * (1 - p)) / sqrt(n) SE ## [1] 0.04974937 Now we can experimentally proof this expected standard error by running a Monte Carlo simulation. We basically create a sampling distribution of size 10000 of the sample proportions with p = 0.45 and n = 100. Only that we lastly subtract p to find out the actual error. test_errors = replicate(10000, mean(sample(c(1,0), replace = T, size = n, prob = c(p, (1 - p))))) # Distribituon of errors test_errors = test_errors - p # The standard deviation of the errors sd(test_errors) ## [1] 0.04960875 # Very close to the theoretical standard error SE ## [1] 0.04974937 And the error distribution is approximatly normal. qqnorm(test_errors);qqline(test_errors) 2.3 Sample Size Resulting in 0.01 SE The maximum SE in relation to proportions is with p = 0.5. Therefore we will take this worst case scenario case to calculate our goal, the required sample size to get a standard error arround 0.01. We can calculate a list of SE based on sample sizes 100 to 5000. p = 0.5 n = 100:5000 list_of_SEs = sqrt(p * (1 - p) / n) head(list_of_SEs, 20) ## [1] 0.05000000 0.04975186 0.04950738 0.04926646 0.04902903 0.04879500 ## [7] 0.04856429 0.04833682 0.04811252 0.04789131 0.04767313 0.04745790 ## [13] 0.04724556 0.04703604 0.04682929 0.04662524 0.04642383 0.04622502 ## [19] 0.04602873 0.04583492 When plotting the list of standard errors we can see at what sample size we will reach a standard error of around 0.01: with a sample size of around 2.500. plot(list_of_SEs, type = &quot;l&quot;) 2.4 Bank and Loans # Number of loans n = 10000 # Probability of default p = 0.03 # Loss per single forclosure loss_per_forclosure = -200000 # Interest_rate x = 0 # Random variable S storing defaults = 1 and non defaults = 0 S = sample(c(0,1), prob = c(1-p, p), size = n, replace = T) head(S, 100) ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ## [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 # Expected value for 10000 loans sum(S * loss_per_forclosure) ## [1] -62600000 # Monte-Carlo simulation amount B = 10000 # Simulation losses = replicate(B, { S = sample(c(0,1), prob = c(1-p, p), size = n, replace = T) sum(S * loss_per_forclosure) }) # Losses distribution from simulation hist(losses / 10^6) # Expected value of S EV = x*(1-p) + loss_per_forclosure*p EV ## [1] -6000 # Standard error of S SE = abs(x - loss_per_forclosure) * sqrt(p * (1-p)) SE ## [1] 34117.44 # Loans of 180000 x = 180000 # loss_per_forclosure*p + x * (1-p) = 0 # Find out x! x = - (loss_per_forclosure * p / (1 - p)) x ## [1] 6185.567 # Interest of 6185.57 dollars for each loan needed to get on average 0 in total back as the bank. EV = loss_per_forclosure * p + x * (1 - p) EV # Correct! ## [1] 0 2.5 Real Polling Data # library(dslabs) data(&quot;polls_us_election_2016&quot;) # Exclude observations that are too old. polls &lt;- polls_us_election_2016 %&gt;% filter(enddate &gt;= &quot;2016-10-31&quot; &amp; state == &quot;U.S.&quot;) head(polls, 5) ## state startdate enddate pollster grade samplesize ## 1 U.S. 2016-11-03 2016-11-06 ABC News/Washington Post A+ 2220 ## 2 U.S. 2016-11-01 2016-11-07 Google Consumer Surveys B 26574 ## 3 U.S. 2016-11-02 2016-11-06 Ipsos A- 2195 ## 4 U.S. 2016-11-04 2016-11-07 YouGov B 3677 ## 5 U.S. 2016-11-03 2016-11-06 Gravis Marketing B- 16639 ## population rawpoll_clinton rawpoll_trump rawpoll_johnson rawpoll_mcmullin ## 1 lv 47.00 43.00 4.00 NA ## 2 lv 38.03 35.69 5.46 NA ## 3 lv 42.00 39.00 6.00 NA ## 4 lv 45.00 41.00 5.00 NA ## 5 rv 47.00 43.00 3.00 NA ## adjpoll_clinton adjpoll_trump adjpoll_johnson adjpoll_mcmullin ## 1 45.20163 41.72430 4.626221 NA ## 2 43.34557 41.21439 5.175792 NA ## 3 42.02638 38.81620 6.844734 NA ## 4 45.65676 40.92004 6.069454 NA ## 5 46.84089 42.33184 3.726098 NA The first poll. Create a confidence interval. n = polls$samplesize[1] x_hat = polls$rawpoll_clinton[1]/100 se_hat = sqrt(x_hat * (1 - x_hat) / n) cf = c(x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat) rm(x_hat, se_hat) Create columns for x_hat, se_hat, lower and upper confidence bounds. Select only the relevant columns. polls = polls %&gt;% mutate(x_hat = polls$rawpoll_clinton/100, se_hat = sqrt(x_hat * (1 - x_hat) / samplesize), lower = x_hat - 1.96 * se_hat, upper = x_hat + 1.96 * se_hat) %&gt;% select(pollster, enddate, x_hat, se_hat, lower, upper) Create a hit column indicating whether our confidence intervals included our true parameter, the final vote count for Clinton 48.2. polls = polls %&gt;% mutate(hit = ifelse(0.482 &gt; lower &amp; 0.482 &lt; upper, TRUE, FALSE)) mean(polls$hit) ## [1] 0.3142857 "],["r-basics.html", "3 R Basics 3.1 Read and Write Data 3.2 Data Types 3.3 Vectors 3.4 Factors 3.5 Matrices and Data Frames 3.6 Lists", " 3 R Basics 3.1 Read and Write Data RStudio UI 1. From Text (base): Standard text based data import (.csv, .txt) 2. From Text (readr): tidyverse version (.csv, .txt) 3. From Excel: tidyxl package used for excel files (.xlsx, etc.) 4. From SPSS: haven package for SPSS files (.sav) 5. From SAS: also haven package for SAS files (.sas) 6. From Stata: also haven package for Stata files (.dta) load() To load or list the available data sets in your system or working directory just write: data(). To then load a dataset, add its name inside: data(data_name). Or you can also load them directly with these functions: as.data.frame() or as.matrix() file.choose() You can also use this function to manually pick out the file from your OS. data = read.csv(file.choose()) help() How to show information for a specific library dataset. 3.1.1 CSV Files A CSV is a simple text based file which has information, line by line: * The first line often contains a header, i.e. names for each column. * Every value is separated by some sort of symbol, like a ,, a ; or “blank spaces” \"\". * Each line represents one row/case in the table afterwards. Suppose you have the following CSV file: mydata.csv name,age,job,city Bob,25,Manager,Seattle Sam,30,Developer,New York read.csv() You can open a file and read its contents by using the read.csv() function specifying its name. It reads the data into a data frame. # Read entire CSV file into a data frame mydata &lt;- read.csv(&quot;mydata.csv&quot;) mydata name age job city 1 Bob 25 Manager Seattle 2 Sam 30 Developer New York File Path When you specify the filename only, it is assumed that the file is located in the current folder. If it is somewhere else, you can specify the exact path that the file is located at. # Specify absolute path like this mydata &lt;- read.csv(&quot;Users/user/data/mydata.csv&quot;) # Web If you want to read CSV Data from the Web, substitute a URL for a file name. The read.csv() functions will read directly from the remote server. mydata &lt;- read.csv(&quot;http://www.example.com/download/mydata.csv&quot;) The read.csv() function automatically coerces non-numeric data into a factor (categorical variable). 3.1.2 DAT Files A file with the .dat file extension is a generic data file that stores specific information relating to the program that created the file. You can open them in R with the read.table() function. 3.1.3 Excel Files Reading excel files in R is done with the readxl package and these functions. Note that excel files often have different sheets. This can be addressed via the sheet parameter in read_excel(). 3.2 Data Types 3.2.1 Convert data types Convert to a specific data type. as.character(object) as.numeric(object) as.integer(object) as.logical(object) 3.2.2 Numeric Doubles Decimal (floating point values) are part of the numeric class in R. n &lt;- 2.2 class(n) typeof(n) is.numeric(n) is.double(n) Integers Natural (whole) numbers are known as integers and are also part of the numeric class. Add an L at the end of each integer. i &lt;- 5L class(i) typeof(i) is.numeric(i) is.integer(i) 3.2.3 Logical Boolean values (True and False) are part of the logical class. In R these are written in All Caps. t &lt;- TRUE class(t) typeof(t) is.logical(t) 3.2.4 Characters Text/string values are known as characters in R. You use quotation marks to create a text character string: char &lt;- &quot;Hello World!&quot; class(char) typeof(char) is.character(char) 3.2.5 NAs The most important note on NAs is that if you compare anything to them, the result is NA. Here is an example. TRUE == FALSE # TRUE is not equal to FALSE 5 &gt; 1 # 5 is bigger than 1 # But note here: comparing TRUE to NA doesn&#39;t result in FALSE, but in NA TRUE == NA So what does that mean when dealing with NA values? What happens if we use a filter in a column that contains NA values ? rev_exp[rev_exp$Expenses == 1130700,] We get our filtered value - but also a whole line of NAs. This happened because, as we have noted, when R checked with the filter the cell containing the NA value, it resulted in an NA. Basically R doesn’t know what to do - does the row pass the filter, or not? R than tells you about it by supplying these NA filled rows. But why doesn’t R just show the whole row and keeps the checked as NA? Because this would imply that the row passed the test, and might be easily mistaken. Note: Blank cells in a row are being processed as complete/filled (““)! When reading/opening the data be sure to set the na.string parameter to”“. Then they are NAs and processed as incomplete, as they in fact are. To show only missing values, we cannot use this solution, because comparing a value to NA only returns NAs and not TRUE or FALSE. rev_exp[rev_exp$Expenses == NA,] Solution You cannot compare anything to NA. So what is the other way? There are two ways - either with !complete.cases() or is.na(). The results are exactly the same. is.na() is just like any other is.() function, such as is.numeric(), it ask what elements in a vector are NA. So either this way with complete.cases(). rev_exp[!complete.cases(rev_exp),] # Shows all rows, regardless of columns, that have NAs rev_exp[!complete.cases(rev_exp$Revenue),] # Shows only rows, in which the Revenue column contain NAs Or this way with is.na(). rev_exp[is.na(rev_exp$Revenue),] # Shows only rows, in which the Revenue column contain NAs rev_exp[!is.na(rev_exp$Revenue),] # Shows only rows, in which the Revenue column contain NAs But this way on the other hand doesn’t work. Not sure why, though… rev_exp[is.na(rev_exp),] 3.2.6 Dates There is a date format in R which can be useful when receiving unformatted date values. today &lt;- Sys.Date() # Function for getting today&#39;s date. class(today) When receiving character type dates we can format them by using the as.Date() function and its format parameter. date &lt;- &quot;13.10.86&quot; date &lt;- as.Date(date, format = &quot;%d.%m.%y&quot;) date class(date) typeof(date) # Interesting. 3.2.7 POSIX Time POSIX is an international and inter operating system time and date format which is very useful in R. The format parameter works like this: in front of every element (day, month etc.) put a %. Then simply write the “time formula” as it is. d = day, m = month, Y = year with four digits, y = year with two digits, H = hour, M = minute. times &lt;- c(&quot;17.09.1991 03:30&quot;, &quot;13.10.1986 05:05&quot;, &quot;16.05.1960 06:06&quot;, &quot;7.07.1961 07:07&quot;) POSIX.Times &lt;- as.POSIXct(times, format = &quot;%d.%m.%Y %H:%M&quot;) POSIX.Times To get help with the format coding. help(&quot;strptime&quot;) 3.3 Vectors Vectors are the basic unit in R. Elements in a vector always have the same data type (just like matrices). 3.3.1 Creating vectors Create a vector (array) from a certain number to another number. 1:10 Even a single element in R is processed as a vector. a &lt;- 1 is.vector(a) # TRUE 3.3.2 Subsetting vectors Indexing works by using brackets and passing the index position of the element as a number. Keep in mind index starts at 1 (in some other programming languages indexing starts at 0). Example of a vector: vector &lt;- 1:5 Access one specific element. vector[3] Access all but one specific element. vector[-3] Access all values but this coherent slice. vector[-(1:4)] Access one coherent slice. vector[2:4] Access specific non-coherent elements. vector[c(1,3,5)] Access elements by name. names(vector) &lt;- &quot;One&quot; vector[&quot;One&quot;] # returns first value. 3.3.3 Naming vectors In R every vector has an accompanying “name vector” which stores the names for each values. To access the name vector you have to use the name() function. By default the names vector is filled with NULL. Then it takes the default 1:n indexing principle. vector &lt;- 1:5 names(vector) To add custom names to the vector (which will then additionally serve a indices), you just have to “fill” the names vector up with your custom name vector. names(vector) &lt;- c(&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;) vector To delete/default the names vector simply set it to NULL. names(vector) &lt;- NULL vector 3.3.4 Filtering vectors Access elements greater or smaller than a specific value. Another way this might be called, is using a filter. vector[vector &gt; 2] Note: It is important to wrap your head around this concept. The filter we add into the indexing brackets consists of the to be filtered vector and an operation with a value. Inside the square brackets a boolean vector is being created. Another way to illustrate goes like this. filter &lt;- vector &gt; 2 filter # a vector with boolean values. Once this boolean vector (i.e. our filter) gets put into the indexing brackets R, it only returns the elements in the “vector” which are TRUE. It is like the boolean filter consists of a vector which has the specific numbers, for example c(3,4,5), to access the to be filtered vector. vector[filter] Another important concept in filtering vectors in R is that you can add several operations. vector[vector &gt; 2 &amp; vector &lt; 4] # Get me all the elements which are bigger than two AND smaller than 4, i.e. 3. 3.3.5 Special Concepts Operations on uneven long vector You can easily calculate vectors with another. This is unlike other languages, where you need loops. vector_one &lt;- 1:10 vector_two &lt;- 1:8 Note: With uneven vectors the smaller one gets refilled. Here element 9 of the first vector gets multiplied by the first element of the second vector. vector_one * vector_two # ... 9 x 1, 10 x 2. 3.4 Factors Further Information In R, factors are used to work with categorical variables, variables that have a fixed and known set of possible values. They are also useful when you want to display character vectors in a non-alphabetical order. Historically, factors were much easier to work with than characters. As a result, many of the functions in base R automatically convert characters to factors. This means that factors often crop up in places where they’re not actually helpful. Imagine that you have a variable that records month: x1 = c(&quot;Dec&quot;, &quot;Apr&quot;, &quot;Jan&quot;, &quot;Mar&quot;) Firstly, there are only twelve possible months, and there’s nothing saving you from typos: x2 &lt;- c(&quot;Dec&quot;, &quot;Apr&quot;, &quot;Jam&quot;, &quot;Mar&quot;) Secondly, it doesn’t sort in a useful way: sort(x1) #&gt; [1] &quot;Apr&quot; &quot;Dec&quot; &quot;Jan&quot; &quot;Mar&quot; You can fix both of these problems with a factor. To create a factor you must start by creating a list of the valid levels: month_levels &lt;- c( &quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot; ) Now you can create a factor: y1 &lt;- factor(x1, levels = month_levels) y1 #&gt; [1] Dec Apr Jan Mar #&gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec sort(y1) #&gt; [1] Jan Mar Apr Dec #&gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec And any values not in the set will be silently converted to NA: y2 &lt;- factor(x2, levels = month_levels) y2 #&gt; [1] Dec Apr &lt;NA&gt; Mar #&gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec If you want a warning, you can use readr::parse_factor(): y2 &lt;- parse_factor(x2, levels = month_levels) #&gt; Warning: 1 parsing failure. #&gt; row col expected actual #&gt; 3 -- value in level set Jam If you omit the levels, they’ll be taken from the data in alphabetical order: factor(x1) #&gt; [1] Dec Apr Jan Mar #&gt; Levels: Apr Dec Jan Mar Sometimes you’d prefer that the order of the levels match the order of the first appearance in the data. You can do that when creating the factor by setting levels to unique(x), or after the fact, with fct_inorder(): f1 &lt;- factor(x1, levels = unique(x1)) f1 #&gt; [1] Dec Apr Jan Mar #&gt; Levels: Dec Apr Jan Mar f2 &lt;- x1 %&gt;% factor() %&gt;% fct_inorder() f2 #&gt; [1] Dec Apr Jan Mar #&gt; Levels: Dec Apr Jan Mar If you ever need to access the set of valid levels directly, you can do so with levels(): levels(f2) #&gt; [1] &quot;Dec&quot; &quot;Apr&quot; &quot;Jan&quot; &quot;Mar&quot; 3.5 Matrices and Data Frames A matrix is basically a table with only one data type stored in it (instead of several as in with data frames). Data frames are tables with different data types, unlike matrices with only one data type. 3.5.1 Creating matrices (1) Create with matrix() Adjust the number of rows/columns. Decide if it should be filled up by rows (FALSE by default, i.e. fills it up by columns). Add names (NULL by default, i.e. none). matrix_object &lt;- matrix(data = 1:10, nrow = 5, ncol = 2, byrow = FALSE, dimnames = NULL ) matrix_object (2) Bind by rows\\ Create a matrix by binding vectors together, row by row. matrix_object &lt;- rbind(c(1:5), c(2:6), c(3:7)) matrix_object (3) Bind by columns\\ Create a matrix by binding vectors together, column by column. matrix_object &lt;- cbind(c(1:5), c(2:6), c(3:7)) matrix_object Note Shorter vectors in cbind() and rbind() fill up according to the longest other vector by restarting from the beginning. E.g., a smaller vector (1:3) starts after 3 at 1 again. 3.5.2 Creating data frames Constructing data frames with data.frame() col_one &lt;- c(&quot;Aruba&quot;, &quot;Afghanistan&quot;, &quot;Angola&quot;, &quot;Albania&quot;, &quot;United Arab Emirates&quot;, &quot;Argentina&quot;) col_two &lt;- (c(&quot;South America&quot;, &quot;Asia&quot;, &quot;Africa&quot;, &quot;Europe&quot;, &quot;Middle East&quot;, &quot;South America&quot;)) col_three &lt;- c(10.244, 35.253, 45.958, 12.877, 7.044, 17.716) countries &lt;- data.frame(country = col_one, region = col_two, birth.rate = col_three) countries 3.5.3 Adding row or columns Adding a new row\\ This isn’t as usual in practice, but it works like this. countries &lt;- rbind(countries, c(1,2,3,4)) countries Adding a new column\\ Add a new column by just writing as if it already exists and than filling it up with a vector. countries$new.column &lt;- c(5,5,5,5,5,5,5) countries Note: Adding a vector which is shorter than the other columns results in replicating the vector until it fills up. But (!), it has to be able to fill it up completely. Otherwise you get an error. 3.5.4 Naming tables Renaming tables works in principle exactly the same as with vectors - by breaking tables down in vectors. There are two name vectors: the row name vector and the column name vector. table &lt;- cbind(c(1:10),c(10:1)) table rownames(table) &lt;- rep(&quot;One&quot;, 10) colnames(table) &lt;- c(&quot;Var1&quot;, &quot;Var2&quot;) Change a specific column name. colnames(starwars)[10] = &quot;planet&quot; # You&#39;d have to find out the column number. It is also possible to name tables columns or rows in rbind(), cbind() and data.frame(). vector_one &lt;- 1:5 vector_two &lt;- 5:1 data_frame &lt;- rbind(v1 = vector_one, v2 = vector_two) data_frame data_frame &lt;- data.frame(country = vector_one, continent = vector_two) # Bind together by columns. data_frame 3.5.5 Deleting rows or columns Delete a column\\ Fill the column up with NULL. countries$new.column &lt;- NULL countries 3.5.6 Rearranging columns Rearrange columns Simple way to rearrange columns in a table. Just overwrite/create a new object by subsetting the columns in a different order by passing in a numerical vector. head(Crime) Crime.Rearranged &lt;- Crime[,c(1,3,2)] head(Crime.Rearranged) 3.5.7 Merging data frames Merging data frames countriesNew &lt;- merge(countries, countries) countriesNew 3.5.8 Subsetting matrices Because tables are two dimensional, they have rows and columns, you have to specify two indices.The first index in the square brackets relates to which row should be targeted. The second index relates to the column. Note: A very important concept in subsetting tables in R is that, if you leave the row or the column index empty, you access the whole row or the whole column. Access several columns in a matrix. matrix_object[ , ] # Shows the whole matrix. matrix_object # Same as above. matrix_object[,1:2] # Shows all rows and only the first and second column. matrix_object[1:10,] # Shows rows 1 to 10 and all columns. matrix_object[,c(1,3)] # Shows all rows and only the first and third column. 3.5.9 Subsetting data frames Data frames can be accessed just like their “cousins” the matrices. They only have one addition: $. Because of the focus on the column names, data frames have a special key symbol to access them: the $ sign. This is how you access it. countries countries$country It is pretty much the same as this, regular, subsetting: countries[,&quot;country&quot;] It just saves time and improves readability. The last thing to understand, when subsetting data frames with $, is if you want to access a certain row you just write the index/name into square brackets by themselves. countries$country[2] The syntax in English therefore would read: “access the country_data_frame object, access its country column, show only the row with the index 2.” 3.5.10 Data frame returns When subsetting rows in a data frame it returns, as might be expected, a data frame. But (!) when subsetting a column, it returns a vector. countries[1,] countries[,1] If you actually to want a data frame returned when subsetting a whole column change the drop property to FALSE. countries[,1, drop = FALSE] 3.5.11 Filtering tables Advanced filtering in data frames can become a bit challenging to read. It is important to keep in mind, that a filter, e.g. this [vector &gt; 1], is basically a boolean vector which picks out all TRUE elements. The no-filters-for-columns logic In data frames you have an additional dimension, therefore, just as in basic subsetting, you must address both the row and column index [row_index, column_index].\\ Important: filters are only used on rows indices! Using a filter on which columns to select doesn’t make sense, because columns have often mixed data types/structures and represent logically different concepts, which cannot be filtered by one singular mathematical operation. This experiment helps to explain the problem. head(countries, n = 3) countries[1,] # Show me the first row and all columns, i.e. all of Aruba&#39;s column. countries[1,] &gt; 5 # Show me the first row and only the columns which are higher than 5. You can see that it checks the cells of row one, column by column, and returns logical values. It only makes sense in the numeric value of birth.rate. If we use this filter to access certain values will get an error. To filter for rows on the other hand works logically and practically fine\\ Filter for rows which have a higher birth rate value than 20 in their Birth.Rate column. countries[countries$birth.rate &gt; 20,] The syntax in English therefore would read: “access the country_data_frame object, filter the rows by those which have a higher value than 20 in their Birth.Rate column, show all columns.”\\ This is the same filter as those two. Just written differently. countries[countries[,&quot;birth.rate&quot;] &gt; 20,] countries[countries[,3] &gt; 20,] Access rows which have a specific column factor. countries[countries$region == &quot;South America&quot;,] # Plain English: show me the rows and all their columns, which have &quot;South America&quot; in their Region column. Access rows with several specific column factors. countries[countries$region == &quot;South America&quot; | countries$region == &quot;Africa&quot;,] # Plain English: show me the rows and all their columns, which have &quot;South America&quot; OR &quot;Africa&quot; in their Region column. 3.6 Lists List are kind of like a storage box in which you can put all types of object inside. Data frames columns and rows are much more limited that way, because vectors are sensitive to the overall length of the data frame and are only limited to vectors. In lists you can put several different data frames inside, a vector with a single element and even plots (and even other lists). This makes it a nice way to organize your data and pass on many different aspects. 3.6.1 Creating lists my_list = list( Letters = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;), RandomNumbers = sample(1:100, 20), TodaysDate = Sys.Date()) # Today&#39;s date. 3.6.2 Adding components my_list$Prices = sample(1:5, 10) 3.6.3 Deleting components my_list$Prices &lt;- NULL Note: In list the numeration gets automatically updated and changed if you delete objects in between two components. They react when changing components. 3.6.4 Renaming components Names can be given while constructing a list with list(). But also via names(). names(my_list)[2] = &quot;NonRandomNumbers&quot; # instead of &quot;RandomNumbers&quot;. my_list$NonRandomNumbers # works! # Not these: names(my_list$RandomNumbers) = &quot;NonRandomNumbers&quot; # changes the components values names. names(Example_list[2]) &lt;- &quot;States&quot; # Same. 3.6.5 Naming inside components names(my_list$TodaysDate) = &quot;Date&quot; # Date # &quot;2021-07-30&quot; 3.6.6 Subsetting list components There are three ways to extract components. First by \\[ \\] Returns the component, but as part of a seperate list. Similar to data frames, when using dataFrame(,1), which also returns a data frame, instead of a vector. my_list[2] typeof(my_list[2]) # list # &gt; $NonRandomNumbers # [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; Second by \\[ \\[ \\] ] Returns the object in the component, e.g. the vector, data frame, plot etc. my_list[[2]] typeof(my_list[[2]]) # Integer vector of NonRandomNumbers. # &gt; my_list[[2]] # [1] 44 88 68 69 30 16 83 2 62 38 15 79 4 52 70 39 98 9 74 86 Third by $ This is exactly the same as the second version, but prettier. my_list$NonRandomNumbers my_list$NonRandomNumbers [1] 67 88 4 15 70 27 92 29 66 91 22 68 13 40 52 24 1 37 64 54 Extracting values in a component Accessing the second element in the first component (“States”). Example_list[[1]][2] # Or even like this. Example_list$States[2] 3.6.7 Filtering lists Once you’ve accessed a component’s object by $ or [ [ ] ] it’s the same as without lists. For example, “Return all values inside NonRandomNumbers that are bigger than 50”: my_list$NonRandomNumbers[my_list$NonRandomNumbers &gt; 50] # &gt; my_list$NonRandomNumbers[my_list$NonRandomNumbers &gt; 50] # [1] 67 88 70 92 66 91 68 52 64 54 "],["packages.html", "4 Packages 4.1 Functionality 4.2 Data 4.3 Visualization", " 4 Packages 4.1 Functionality 4.1.1 kableExtra The goal of kableExtra is to help you build common complex tables and manipulate table styles. Plots nice tables, basically. It imports the pipe %&gt;% symbol from magrittr and verbalize all the functions, so basically you can add “layers” to a kable output in a way that is similar with ggplot2 and plotly. 4.1.2 gridExtra : Used to arrange plots next to each other. 4.1.3 kableExtra : The goal of kableExtra is to help you build common complex tables and manipulate table styles. Plots nice tables, basically. It imports the pipe %&gt;% symbol from magrittr and verbalize all the functions, so basically you can add “layers” to a kable output in a way that is similar with ggplot2 and plotly. 4.1.4 unpivotr Tools for converting data from complex or irregular layouts to a columnar structure. For example, tables with multilevel column or row headers, or spreadsheets. 4.1.5 tibble tible provides a ‘tbl\\_df’ class (the ‘tibble’) that provides stricter checking and better formatting than the traditional data frame. 4.1.6 dslabs 26 Datasets and some functions for data analysis. Used to practice data visualization, statistical inference, modeling, linear regression, data wrangling and machine learning. 4.1.7 knitr Engine for dynamic report generation with R. Enables integration of R code into LaTeX, LyX, HTML, Markdown, AsciiDoc, and reStructuredText documents. The purpose of knitr is to allow reproducible research in R through the means of Literate Programming. 4.1.8 readr The goal of ‘readr’ is to provide a fast and friendly way to read rectangular data (like ‘csv,’ ‘tsv,’ and ‘fwf’) 4.1.9 readxl The readxl package makes it easy to get data out of Excel and into R. 4.1.10 tidyxl Imports non-tabular data from Excel files into R. It exposes cell content, position, formatting and comments in a tidy structure for further manipulation, especially by the unpivotr package. 4.1.11 corrgram Create correlograms from data frames directly. 4.1.12 corrplot Create correlograms from preprocessed data frames. Needs a matrix with correlations between each variable. 4.1.13 rtweet Collect and organize Twitter data. 4.1.14 caTools Contains several basic utility functions including: moving (rolling, running) window statistic functions, read/write for GIF and ENVI binary files, fast calculation of AUC, LogitBoost classifier, base64 encoder/decoder, round-off-error-free sum and cumsum, etc. 4.2 Data 4.2.1 ggplot2movies IMDB movies data set useful to experiment with ggplot2 visualizations. 4.2.2 WDI Search and download data from over 40 databases hosted by the World Bank, including the World Development Indicators (‘WDI’), International Debt Statistics, Doing Business, Human Capital Index, and Sub-national Poverty indicators, GDP, Population. 4.2.3 essurvey Package used to easily download specific European Social Survey data. 4.2.4 wbstats This package allows to download data from the world bank database. library(wbstats) wb_cachelist$indicators ## # A tibble: 16,649 × 8 ## indicator_id indicator unit indicator_desc source_org topics source_id ## &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 1.0.HCount.1… Poverty H… NA The poverty hea… LAC Equity … &lt;df [… 37 ## 2 1.0.HCount.2… Poverty H… NA The poverty hea… LAC Equity … &lt;df [… 37 ## 3 1.0.HCount.M… Middle Cl… NA The poverty hea… LAC Equity … &lt;df [… 37 ## 4 1.0.HCount.O… Official … NA The poverty hea… LAC Equity … &lt;df [… 37 ## 5 1.0.HCount.P… Poverty H… NA The poverty hea… LAC Equity … &lt;df [… 37 ## 6 1.0.HCount.V… Vulnerabl… NA The poverty hea… LAC Equity … &lt;df [… 37 ## 7 1.0.PGap.1.9… Poverty G… NA The poverty gap… LAC Equity … &lt;df [… 37 ## 8 1.0.PGap.2.5… Poverty G… NA The poverty gap… LAC Equity … &lt;df [… 37 ## 9 1.0.PGap.Poo… Poverty G… NA The poverty gap… LAC Equity … &lt;df [… 37 ## 10 1.0.PSev.1.9… Poverty S… NA The poverty sev… LAC Equity … &lt;df [… 37 ## # … with 16,639 more rows, and 1 more variable: source &lt;chr&gt; wb_cachelist$topics ## # A tibble: 21 × 3 ## topic_id topic topic_desc ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Agriculture &amp; Rural Development &quot;For the 70 percent of the world&#39;s … ## 2 2 Aid Effectiveness &quot;Aid effectiveness is the impact th… ## 3 3 Economy &amp; Growth &quot;Economic growth is central to econ… ## 4 4 Education &quot;Education is one of the most power… ## 5 5 Energy &amp; Mining &quot;The world economy needs ever-incre… ## 6 6 Environment &quot;Natural and man-made environmental… ## 7 7 Financial Sector &quot;An economy&#39;s financial markets are… ## 8 8 Health &quot;Improving health is central to the… ## 9 9 Infrastructure &quot;Infrastructure helps determine the… ## 10 10 Social Protection &amp; Labor &quot;The supply of labor available in a… ## # … with 11 more rows # result = wb_search(&quot;&quot;) # result$indicator_desc # Takes a long time to download data = wb_data(&quot;SP.POP.TOTL&quot;, start_date = 1960, end_date = 2020) head(data) ## # A tibble: 6 × 9 ## iso2c iso3c country date SP.POP.TOTL unit obs_status footnote last_updated ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 AF AFG Afghanis… 2020 38928341 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2021-12-16 ## 2 AF AFG Afghanis… 2019 38041757 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2021-12-16 ## 3 AF AFG Afghanis… 2018 37171922 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2021-12-16 ## 4 AF AFG Afghanis… 2017 36296111 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2021-12-16 ## 5 AF AFG Afghanis… 2016 35383028 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2021-12-16 ## 6 AF AFG Afghanis… 2015 34413603 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2021-12-16 # Example visualization data %&gt;% filter(country == &quot;Germany&quot;) %&gt;% ggplot(aes(date, SP.POP.TOTL/1000000)) + geom_line() 4.3 Visualization 4.3.1 igraph Creating and manipulating graphs and analyzing networks. It is written in C and also exists as Python and R packages. 4.3.2 ggthemes 4.3.3 ggrepel This geometry adds “smart” labels to each data point, meaining labels that “repel” each other automaticaly to not overlap each other. Sometimes the data points are to close to each other. In these cases one solution might be to use a log scale to stretch those clustered observation away from each other. data(murders) murders %&gt;% ggplot(aes(population,total)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_text(aes(label = abb)) murders %&gt;% ggplot(aes(population,total)) + geom_point() + scale_x_log10() + scale_y_log10() + ggrepel::geom_text_repel(aes(label = abb)) 4.3.4 ggridges Density Ridges In cases in which we are concerned that the boxplot summary is too simplistic, we can show stacked smooth densities or histograms. We refer to these as ridge plots. Because we are used to visualizing densities with values in the x-axis, we stack them vertically. Also, because more space is needed in this approach, it is convenient to overlay them. The package ggridges provides a convenient function for doing this. Here is the income data shown above with boxplots but with a ridge plot. gapminder %&gt;% filter(year == 2015) %&gt;% ggplot(aes(life_expectancy,continent, fill = continent)) + ggridges::geom_density_ridges(show.legend = F) ## Picking joint bandwidth of 2.23 4.3.5 kableExtra Plots the most simple table. mtcars[1:10,] %&gt;% kbl() mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 4.3.6 ?gridExtra https://cran.r-project.org/web/packages/gridExtra/vignettes/arrangeGrob.html There are often reasons to graph plots next to each other. The gridExtra package permits us to do that with grid.arrange(): library(gridExtra) p1 &lt;- plot(mtcars$mpg) p2 &lt;- plot(mtcars$cyl) # grid.arrange(p1, p2, ncol = 2) "],["rmarkdown.html", "5 Rmarkdown", " 5 Rmarkdown "],["tidyverse.html", "6 Tidyverse 6.1 dplyr 6.2 forcats 6.3 purrr", " 6 Tidyverse 6.1 dplyr dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges: select() picks variables based on their names. rename() renames variables names. filter() picks cases based on their values. arrange() changes the ordering of the rows. slice() picks cases based on their indices. mutate() adds new variables that are functions of existing variables. relocate() changes variable positions. summarise() reduces multiple values down to a single summary. group_by() groups tibble in order to calculate grouped statistics with summarise(). pull() pulls the vector value from a summarize tibble. 6.1.1 Pipe With dplyr we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %&gt;%. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects. starwars %&gt;% # Pass on starwars dataset... select(name, height, mass) %&gt;% # ... select name, height, mass variables and pass on... rename(kilograms = mass) %&gt;% # ... rename mass to kilograms and pass on... filter(kilograms &gt; 50) # filter cases bigger than 50 kilograms. In general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example: 16 %&gt;% sqrt() #&gt; [1] 4 We can continue to pipe values along: 16 %&gt;% sqrt() %&gt;% log2() #&gt; [1] 2 The above statement is equivalent to log2(sqrt(16)). Remember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined: 16 %&gt;% sqrt() %&gt;% log(base = 2) #&gt; [1] 2 Therefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. 6.1.2 Select https://dplyr.tidyverse.org/reference/select.html Often you work with large datasets with many columns but only a few are actually of interest to you. select() allows you to rapidly zoom in on a useful subset. starwars %&gt;% select(name, birth_year, homeworld) # A tibble: 3 x 3 # name birth_year homeworld # &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; # 1 Luke Skywalker 19 Tatooine # 2 C-3PO 112 Tatooine # 3 R2-D2 33 Naboo Equivalent to: starwars[,c(&quot;name&quot;,&quot;birth_year&quot;,&quot;homeworld&quot;)] 6.1.3 Rename https://dplyr.tidyverse.org/reference/rename.html You could also rename variables in select(), but because it drops all the variables not explicitly mentioned, it’s not that useful. Instead, use rename(): starwars %&gt;% rename(planet = homeworld) # new_name = variable_name # A tibble: 3 x 3 # name birth_year planet # &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; # 1 Luke Skywalker 19 Tatooine # 2 C-3PO 112 Tatooine # 3 R2-D2 33 Naboo Equivalent to: colnames(starwars)[10] = &quot;planet&quot; # You&#39;d have to find out the column number. 6.1.4 Filter https://dplyr.tidyverse.org/reference/filter.html starwars %&gt;% filter(eye_color == &quot;blue&quot;, mass &lt; 77) # A tibble: 3 x 3 # name height mass hair_color skin_color eye_color # &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; # 1 Beru Whi… 165 75 brown light blue # 2 Adi Gall… 184 50 none dark blue # 3 Luminara… 170 56.2 black yellow blue Roughly the equivalent to: starwars[starwars$eye_color == &quot;blue&quot; &amp; starwars$mass &lt; 77, ] 6.1.5 Arrange https://dplyr.tidyverse.org/reference/arrange.html This function arranges tables by variables. It takes a data frame, and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns: starwars %&gt;% arrange(mass) # A tibble: 3 x 3 # name height mass # &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; # 1 Ratts… 79 15 # 2 Yoda 66 17 # 3 Wicke… 88 20 Use desc()to order a column in descending order: starwars %&gt;% arrange(desc(mass)) # A tibble: 3 x 3 # name height mass # &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; # 1 Jabba… 175 1358 # 2 Griev… 216 159 # 3 IG-88 200 140 6.1.6 Slice https://dplyr.tidyverse.org/reference/slice.html slice() lets you index rows by their (integer) locations. It allows you to select, remove, and duplicate rows. starwars %&gt;% slice(1:3) # A tibble: 3 x 5 # name height mass hair_color skin_color # &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; # 1 Luke … 172 77 blond fair # 2 C-3PO 167 75 NA gold # 3 R2-D2 96 32 NA white, blue Other Slices Slice is accompanied by a number of helpers for common use cases: * slice_head() and slice_tail() select the first or last rows. * slice_sample() randomly selects rows. * slice_min() and slice_max() select rows with highest or lowest values of a variable. If .data is a grouped_df, the operation will be performed on each group, so that (e.g.) slice_head(df, n = 5) will select the first five rows in each group. Examples:  https://dplyr.tidyverse.org/reference/slice.html  6.1.7 Mutate https://dplyr.tidyverse.org/reference/mutate.html Besides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. This creates a new column with height in meters instead of centimeters: starwars %&gt;% mutate(height_m = height / 100) %&gt;% relocate(height_m, .before = height) # Move it before variable height, to the beginning. # A tibble: 87 x 15 # name height_m height # &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; # 1 Luke Sky… 1.72 172 # 2 C-3PO 1.67 167 # 3 R2-D2 0.96 96 If you only want to keep the new variables, use transmute(). 6.1.8 Relocate https://dplyr.tidyverse.org/reference/relocate.html Use a similar syntax as select() to move blocks of columns at once. starwars %&gt;% relocate(sex:gender, .before = height) # Move all variables between sex and gender before variable height. # A tibble: 3 x 4 # name sex gender height # &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; # 1 Luke … male mascu… 172 # 2 C-3PO none mascu… 167 # 3 R2-D2 none mascu… 96 Another syntax that relocates the new variable up front would be: starwars %&gt;% mutate(height_m = height / 100) %&gt;% select(height_m, everything()) # everything() return all other variables. 6.1.9 Summarise https://dplyr.tidyverse.org/reference/summarise.html It collapses a data frame to a single row. It’s not that useful until we learn the group_by() verb below. This summarise syntax returns the mean and standard deviation for all cases, across species, genders, sex etc. : starwars %&gt;% summarise(mean_height = mean(height, na.rm = T), mean_mass = mean(mass, na.rm = T)) # A tibble: 1 x 2 # mean_height mean_mass # &lt;dbl&gt; &lt;dbl&gt; # 1 174. 97.3 Summarize with group by Most data operations are done on groups defined by variables. group_by() takes an existing tbl and converts it into a grouped tbl where operations are performed “by group.” ungroup() removes grouping. starwars %&gt;% group_by(sex) %&gt;% # Creates a tibble with a group: sex [5] - 5 levels summarise( # Create summary statistics for height and mass, grouped by sex variable. height = mean(height, na.rm = TRUE), mass = mean(mass, na.rm = TRUE)) # A tibble: 5 x 3 # sex height mass # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 female 169. 54.7 # 2 hermaphroditic 175 1358 # 3 male 179. 81.0 # 4 none 131. 69.8 # 5 NA 181. 48 More examples of grouped summary statistics: starwars %&gt;% group_by(sex) %&gt;% summarise( n = n(), min_mass = min(mass, na.rm = T), mean_mass = mean(mass, na.rm = T), sd_mass = sd(mass, na.rm = T), max_mass = max(mass, na.rm = T) ) # A tibble: 5 x 6 # sex n min_mass mean_mass sd_mass max_mass # &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 male 60 15 81.0 28.2 159 # 2 female 16 45 54.7 8.59 75 # 3 none 6 32 69.8 51.0 140 # 4 NA 4 48 48 NA 48 # 5 hermaphroditic 1 1358 1358 NA 1358 6.1.10 Pull https://dplyr.tidyverse.org/reference/pull.html A useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. starwars %&gt;% group_by(sex) %&gt;% summarise(height = mean(height, na.rm = TRUE)) %&gt;% pull(height) # [1] 169.2667 175.0000 179.1053 131.2000 181.3333 6.1.11 Distinct values https://dplyr.tidyverse.org/reference/n_distinct.html Function returns the number of distinct values in a column. Similar to levels in factors. n_distinct(starwars$sex) #[1] 5 6.1.12 Counts and proportions https://dplyr.tidyverse.org/reference/count.html Function lets you quickly count the unique values of one or more variables. starwars %&gt;% count(eye_color, sort = T) # A tibble: 15 × 2 # eye_color n # &lt;chr&gt; &lt;int&gt; # 1 brown 21 # 2 blue 19 # 3 yellow 11 # 4 black 10 # 5 orange 8 With two variables, count returns the number of times each combination of possible values has happened. starwars %&gt;% count(eye_color, hair_color, sort = T, name = &quot;count&quot;) # A tibble: 35 × 3 # eye_color hair_color count # &lt;chr&gt; &lt;chr&gt; &lt;int&gt; # 1 black none 9 # 2 brown black 9 # 3 brown brown 9 # 4 blue brown 7 # 5 orange none 7 # 6 yellow none 6 # 7 blue blond 3 # 8 blue none 3 # 9 red none 3 # 10 blue black 2 # … with 25 more rows Count and Prop Tables Simply mutate a frequency and percentage column on a counted table. starwars %&gt;% count(sex) %&gt;% mutate(freq = n / sum(n)) %&gt;% mutate(perc = freq * 100) # A tibble: 5 × 4 # sex n freq perc # &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 female 16 0.184 18.4 # 2 hermaphroditic 1 0.0115 1.15 # 3 male 60 0.690 69.0 # 4 none 6 0.0690 6.90 # 5 NA 4 0.0460 4.60 6.1.13 Case when https://dplyr.tidyverse.org/reference/case_when.html The case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0: x &lt;- c(-2, -1, 0, 1, 2) case_when(x &lt; 0 ~ &quot;Negative&quot;, x &gt; 0 ~ &quot;Positive&quot;, TRUE ~ &quot;Zero&quot;) #&gt; [1] &quot;Negative&quot; &quot;Negative&quot; &quot;Zero&quot; &quot;Positive&quot; &quot;Positive&quot; A common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this: murders %&gt;% mutate(group = case_when( abb %in% c(&quot;ME&quot;, &quot;NH&quot;, &quot;VT&quot;, &quot;MA&quot;, &quot;RI&quot;, &quot;CT&quot;) ~ &quot;New England&quot;, abb %in% c(&quot;WA&quot;, &quot;OR&quot;, &quot;CA&quot;) ~ &quot;West Coast&quot;, region == &quot;South&quot; ~ &quot;South&quot;, TRUE ~ &quot;Other&quot;)) %&gt;% group_by(group) %&gt;% summarize(rate = sum(total) / sum(population) * 10^5) #&gt; # A tibble: 4 x 2 #&gt; group rate #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 New England 1.72 #&gt; 2 Other 2.71 #&gt; 3 South 3.63 #&gt; 4 West Coast 2.90 6.1.14 Between https://dplyr.tidyverse.org/reference/between.html A common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type x &gt;= a &amp; x &lt;= b However, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation. between(x, a, b) Or on a tibble using filter. filter(starwars, between(height, 100, 150)) #&gt; # A tibble: 5 x 14 #&gt; name height mass hair_color skin_color eye_color birth_year sex gender #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Leia Or… 150 49 brown light brown 19 fema… femini… #&gt; 2 Mon Mot… 150 NA auburn fair blue 48 fema… femini… #&gt; 3 Watto 137 NA black blue, grey yellow NA male mascul… #&gt; 4 Sebulba 112 40 none grey, red orange NA male mascul… #&gt; 5 Gasgano 122 NA none white, bl… black NA male mascul… #&gt; # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, #&gt; # vehicles &lt;list&gt;, starships &lt;list&gt; 6.2 forcats https://forcats.tidyverse.org The goal of the forcats package is to provide a suite of useful tools that solve common problems with factors. Factors are useful when you have categorical data, variables that have a fixed and known set of values, and when you want to display character vectors in non-alphabetical order. 6.2.1 Count factors fct_count() https://forcats.tidyverse.org/reference/fct_count.html Calculates a count and prop table. starwars$sex %&gt;% factor() %&gt;% fct_count(sort = T, prop = T) # A tibble: 5 × 3 # f n p # &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; # 1 male 60 0.690 # 2 female 16 0.184 # 3 none 6 0.0690 # 4 NA 4 0.0460 # 5 hermaphroditic 1 0.0115 fct_match() https://forcats.tidyverse.org/reference/fct_match.html Test for presence of levels in a factor. Do any of lvls occur in f? table(fct_match(gss_cat$marital, c(&quot;Married&quot;, &quot;Divorced&quot;))) # Do those levels exist in marital column? Return cases where it&#39;s TRUE and where it&#39;s FALSE. FALSE TRUE 7983 13500 6.2.2 Reorder levels fct_inorder() When using factor(), by default, it returns levels in ascending order. f &lt;- factor(c(&quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;)) f \\[1\\] b b a c c c Levels: a b c Explicitly order levels in the order the observations appear. fct_inorder(f) #&gt; [1] b b a c c c #&gt; Levels: b a c fct_infreq() Order the levels by their number of observations. fct_infreq(f) #&gt; [1] b b a c c c #&gt; Levels: c b a fct_inseq() Changes the levels to be in order, meaning in ascending sequence. Useful if you have a factor with unordered levels. f = factor(1:3, levels = c(3,2,1)) f # [1] 1 2 3 # Levels: 3 2 1 fct_inseq(f) # [1] 1 2 3 # Levels: 1 2 3 fct_relevel() https://forcats.tidyverse.org/reference/fct_relevel.html Move any number of levels to any location. In its simplest form, it pushes each consecutive level entered to the front. f &lt;- factor(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), levels = c(&quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;a&quot;)) fct_relevel(f, &quot;a&quot;) # Push &quot;a&quot; to the first place #&gt; [1] a b c d Move level to a specific position. fct_relevel(f, &quot;a&quot;, after = 2) #&gt; [1] a b c d #&gt; Levels: b c a d Move level to the end. fct_relevel(f, &quot;a&quot;, after = Inf) #&gt; [1] a b c d #&gt; Levels: b c d a fct_rev() https://forcats.tidyverse.org/reference/fct_rev.html Reverse order of levels. Often useful when plotting. f &lt;- factor(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) fct_rev(f) #&gt; [1] a b c #&gt; Levels: c b a fct_shift() Shift factor levels to left or right, wrapping around at end. x &lt;- factor( c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;), levels = c(&quot;Sun&quot;, &quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;)) fct_shift(x,1) # Shift levels 1 to the left, i.e. Sun is at last position. # [1] Mon Tue Wed # Levels: Mon Tue Wed Thu Fri Sat Sun fct_shift(x,-2) # Shift levels 2 to the right, i.e. Fri comes out at first position. # [1] Mon Tue Wed # Levels: Fri Sat Sun Mon Tue Wed Thu fct_shuffle() Randomly shuffle order of levels. f &lt;- factor(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) fct_shuffle(f) #&gt; [1] a b c #&gt; Levels: b c a fct_reorder https://forcats.tidyverse.org/reference/fct_reorder.html Reorder factor levels by sorting along another variable. df &lt;- tibble::tribble( ~color, ~a, ~b, &quot;blue&quot;, 1, 2, &quot;green&quot;, 6, 2, &quot;purple&quot;, 3, 3, &quot;red&quot;, 2, 3, &quot;yellow&quot;, 5, 1 ) Reorders the first by the second argument. df$color &lt;- factor(df$color) # color is our example factor fct_reorder(df$color, df$a, min) \\[1\\] blue green purple red yellow Levels: blue red purple yellow green fct_reorder2() Reorders the factor by the y values associated with the largest x values. This makes the plot easier to read because the line colours line up with the legend. fct_reorder2(df$color, df$a, df$b) #&gt; [1] blue green purple red yellow #&gt; Levels: purple red blue green yellow This is very useful when plotting. boxplot(Sepal.Width ~ fct_reorder(Species, Sepal.Width), data = iris) # Descending order: boxplot(Sepal.Width ~ fct_reorder(Species, Sepal.Width, .desc = TRUE), data = iris) 6.3 purrr https://purrr.tidyverse.org Applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. 6.3.1 Map https://purrr.tidyverse.org/reference/map.html The first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list: x = 1:5 add_one = function(value) { value + 1 } map(x, add_one) # [[1]] # [1] 2 # [[2]] # [1] 3 # [[3]] # [1] 4 # [[4]] # [1] 5 # [[5]] # [1] 6 6.3.2 Map doubles If we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values. x = 1:5 map_dbl(x, add_one) # [1] 2 3 4 5 6 6.3.3 Map dataframes 6.3.4 Map functions/closures Instead of creating and storing a separate function which we’ll apply via mapin the next step like this: x = 1:5 add_one = function(value) { value + 1 } map_dbl(x, add_one) # [1] 2 3 4 5 6 ` We can simply create an &quot;anonymous function&quot; inside the `map` function. `r map_dbl(x, function(x) x + 1) # [1] 2 3 4 5 6 "],["visualizationggplot2.html", "7 Visualization/ggplot2 7.1 gg object 7.2 Aesthetics 7.3 Geometries 7.4 Coordinates 7.5 Tiles 7.6 Scales 7.7 Labels", " 7 Visualization/ggplot2 One reason ggplot2 is generally more intuitive for beginners is that it uses a grammar of graphics, the gg in ggplot2. This is analogous to the way learning grammar can help a beginner construct hundreds of different sentences by learning just a handful of verbs, nouns and adjectives without having to memorize each specific sentence. One limitation is that ggplot2 is designed to work exclusively with data tables in tidy format (where rows are observations and columns are variables). However, a substantial percentage of datasets that beginners work with are in, or can be converted into, this format. An advantage of this approach is that, assuming that our data is tidy, ggplot2 simplifies plotting code and the learning of grammar for a variety of plots. The first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the murders in the US plot and introduce some of the ggplot2 terminology. The main three components to note are: Data: The US murders data table is being summarized. We refer to this as the data component. Geometry: The plot is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, and boxplot. Aesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using. 7.1 gg object ggplot() initializes a ggplot object. It can be used to declare the input data frame for a graphic and to specify the set of plot aesthetics intended to be common throughout all subsequent layers unless specifically overridden. ggplot(data = murders) We can also pipe the data in as the first argument. So this line of code is equivalent to the one above: murders %&gt;% ggplot() 7.2 Aesthetics Aesthetic mappings describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The aes function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the aes function is often used as the argument of a geometry function. This example produces a scatterplot of total murders versus population in millions: murders %&gt;% ggplot() + geom_point(aes(x = population / 10^6, y = total)) We can drop the x = and y = if we wanted to since these are the first and second expected arguments, as seen in the help page. Instead of defining our plot from scratch, we can also add a layer to the p object that was defined above as p &lt;- ggplot(data = murders): p + geom_point(aes(population / 10^6, total)) The scale and labels are defined by default when adding this layer. Like dplyr functions, aes also uses the variable names from the object component: we can use population and total without having to call them as murders$population and murders$total. The behavior of recognizing the variables from the data component is quite specific to aes. With most functions, if you try to access the values of population or total outside of aes you receive an error. colour and fill Almost every geom has either colour, fill, or both. They can be specified in the following ways: * A name, e.g., “red.” R has 657 built-in named colours, which can be listed with colours(). * An rgb specification, with a string of the form “#RRGGBB” where each of the pairs RR, GG, BB consists of two hexadecimal digits giving a value in the range 00 to FF * You can optionally make the colour transparent by using the form “#RRGGBBAA.” * An NA, for a completely transparent colour. * The munsell package, by Charlotte Wickham, makes it easy to choose specific colours using a system designed by Albert H. Munsell. If you invest a little in learning the system, it provides a convenient way of specifying aesthetically pleasing colours. lines As well as colour, the appearance of a line is affected by size, linetype, linejoin and lineend. Line type https://ggplot2.tidyverse.org/articles/ggplot2-specs.html#sec:line-type-spec Line types can be specified with an integer or name: 0 = blank, 1 = solid, 2 = dashed, 3 = dotted, 4 = dotdash, 5 = longdash, 6 = twodash, as shown below: Size The size of a line is its width in mm. Line end/join paramters https://ggplot2.tidyverse.org/articles/ggplot2-specs.html#line-endjoin-paramters. The appearance of the line end is controlled by the lineend paramter, and can be one of “round,” “butt” (the default), or “square.” The appearance of line joins is controlled by linejoin and can be one of “round” (the default), “mitre,” or “bevel.” Mitre joins are automatically converted to bevel joins whenever the angle is too small (which would create a very long bevel). This is controlled by the linemitre parameter which specifies the maximum ratio between the line width and the length of the mitre. Polygons The border of the polygon is controlled by the colour, linetype, and size aesthetics as described above. The inside is controlled by fill. Point Shape https://ggplot2.tidyverse.org/articles/ggplot2-specs.html#sec:shape-spec Shapes take five types of values: * An integer in \\[0:25\\] * The name of the shape: “circle,” “dot,” “square,” “square filled”… * A single character, to use that character as a plotting symbol. * A . to draw the smallest rectangle that is visible, usualy 1 pixel. * An NA, to draw nothing. Colour and fill https://ggplot2.tidyverse.org/articles/ggplot2-specs.html#colour-and-fill-1 Note that shapes 21-24 have both stroke colour and a fill. The size of the filled part is controlled by size, the size of the stroke is controlled by stroke. Each is measured in mm, and the total size of the point is the sum of the two. Note that the size is constant along the diagonal in the following figure. Text Font face https://ggplot2.tidyverse.org/articles/ggplot2-specs.html#font-face There are only three fonts that are guaranteed to work everywhere: “sans” (the default), “serif,” or “mono.” It’s trickier to include a system font on a plot because text drawing is done differently by each graphics device (GD). There are five GDs in common use (png(), pdf(), on screen devices for Windows, Mac and Linux), so to have a font work everywhere you need to configure five devices in five different ways. Two packages simplify the quandary a bit: showtext makes GD-independent plots by rendering all text as polygons. extrafont converts fonts to a standard format that all devices can use. Both approaches have pros and cons, so you will to need to try both of them and see which works best for your needs. Font size The size of text is measured in mm. This is unusual, but makes the size of text consistent with the size of lines and points. Typically you specify font size using points (or pt for short), where 1 pt = 0.35mm. ggplot2 provides this conversion factor in the variable .pt, so if you want to draw 12pt text, set size = 12 / .pt. 7.3 Geometries In ggplot2 we create graphs by adding layers. Layers can define geometries, compute summary statistics, define what scales to use, or even change styles. To add layers, we use the symbol +. In general, a line of code will look like this: # DATA %&gt;% ggplot() + LAYER 1 + LAYER 2 + … + LAYER N 7.3.1 Points starwars %&gt;% ggplot(aes(mass, height, color = sex)) + geom_point() + scale_x_continuous(trans = &quot;log10&quot;) + scale_y_continuous(trans = &quot;log10&quot;) ## Warning: Removed 28 rows containing missing values (geom_point). The point geom is used to create scatterplots. The scatterplot is most useful for displaying the relationship between two continuous variables. It can be used to compare one continuous and one categorical variable, or two categorical variables, but a variation like geom_jitter(), geom_count(), or geom_bin2d() is usually more appropriate. A bubblechart is a scatterplot with a third variable mapped to the size of points. starwars %&gt;% ggplot(aes(sex, height)) + geom_point() ## Warning: Removed 6 rows containing missing values (geom_point). starwars %&gt;% ggplot(aes(sex, height, color = sex)) + geom_jitter(show.legend = NA) + theme(legend.position=&quot;none&quot;) ## Warning: Removed 6 rows containing missing values (geom_point). Arguments geom_point( mapping = NULL, data = NULL, stat = &quot;identity&quot;, position = &quot;identity&quot;, ..., na.rm = FALSE, show.legend = NA, inherit.aes = TRUE ) Aesthetics geom_point() understands the following aesthetics (required aesthetics are in bold): * x ! * y ! * alpha * colour * fill * group * shape * size * stroke Overplotting The biggest potential problem with a scatterplot is overplotting: whenever you have more than a few points, points may be plotted on top of one another. This can severely distort the visual appearance of the plot. There is no one solution to this problem, but there are some techniques that can help. You can add additional information with geom_smooth(), geom_quantile() or geom_density_2d(). If you have few unique x values, geom_boxplot() may also be useful. Alternatively, you can summarise the number of points at each location and display that in some way, using geom_count(), geom_hex(), or geom_density2d(). Another technique is to make the points transparent (e.g. geom_point(alpha = 0.05)) or very small (e.g. geom_point(shape = “.”)). Examples https://ggplot2.tidyverse.org/reference/geom\\_point.html#examples 7.3.2 Text  https://ggplot2.tidyverse.org/reference/geom_text.html  mtcars %&gt;% ggplot() + aes(wt, mpg, label = rownames(mtcars)) + # labels can be accessed via rownames. geom_text(check_overlap = T) # text data points often overlap, so turn on check_overlap. Test Geom Text geoms are useful for labeling plots. They can be used by themselves as scatterplots or in combination with other geoms, for example, for labeling points or for annotating the height of bars. geom_text() adds only text to the plot. geom_label() draws a rectangle behind the text, making it easier to read. Arguments geom_text( mapping = NULL, data = NULL, stat = &quot;identity&quot;, position = &quot;identity&quot;, ..., parse = FALSE, nudge_x = 0, # Nudge each point to left or right. nudge_y = 0, # Nudge each point to top or bottom. check_overlap = FALSE, # If TRUE, text that overlaps previous text in the same layer will not be plotted na.rm = FALSE, show.legend = NA, inherit.aes = TRUE ) geom_label( mapping = NULL, data = NULL, stat = &quot;identity&quot;, position = &quot;identity&quot;, ..., parse = FALSE, nudge_x = 0, # Nudge each point to left or right. nudge_y = 0, # Nudge each point to top or bottom. label.padding = unit(0.25, &quot;lines&quot;), # Amount of padding around label. label.r = unit(0.15, &quot;lines&quot;), # Radius of rounded corners. label.size = 0.25, # Size of label border. na.rm = FALSE, show.legend = NA, inherit.aes = TRUE ) Aesthetics geom_text() understands the following aesthetics (required aesthetics are in bold): * x ! * y ! * label ! * alpha * angle * colour * family * fontface * group * hjust * lineheight * size * vjust Alignment You can modify text alignment with the vjust and hjust aesthetics. These can either be a number between 0 (right/bottom) and 1 (top/left) or a character (“left,” “middle,” “right,” “bottom,” “center,” “top”). There are two special alignments: “inward” and “outward.” Inward always aligns text towards the center, and outward aligns it away from the center. Examples https://ggplot2.tidyverse.org/reference/geom\\_text.html#examples 7.3.3 Bars mpg %&gt;% ggplot() + aes(class) + geom_bar() Bar Geom There are two types of bar charts: geom_bar() and geom_col(). geom_bar() makes the height of the bar proportional to the number of cases in each group (or if the weight aesthetic is supplied, the sum of the weights). If you want the heights of the bars to represent values in the data, use geom_col() instead. geom_bar() uses stat_count() by default: it counts the number of cases at each x position. geom_col() uses stat_identity(): it leaves the data as is. geom_bar( mapping = NULL, data = NULL, stat = &quot;count&quot;, position = &quot;stack&quot;, ..., width = NULL, na.rm = FALSE, orientation = NA, show.legend = NA, inherit.aes = TRUE ) geom_col( mapping = NULL, data = NULL, position = &quot;stack&quot;, ..., width = NULL, na.rm = FALSE, show.legend = NA, inherit.aes = TRUE ) stat_count( mapping = NULL, data = NULL, geom = &quot;bar&quot;, position = &quot;stack&quot;, ..., width = NULL, na.rm = FALSE, orientation = NA, show.legend = NA, inherit.aes = TRUE ) Details A bar chart uses height to represent a value, and so the base of the bar must always be shown to produce a valid visual comparison. Proceed with caution when using transformed scales with a bar chart. It’s important to always use a meaningful reference point for the base of the bar. For example, for log transformations the reference point is 1. In fact, when using a log scale, geom_bar() automatically places the base of the bar at 1. Furthermore, never use stacked bars with a transformed scale, because scaling happens before stacking. As a consequence, the height of bars will be wrong when stacking occurs with a transformed scale. By default, multiple bars occupying the same x position will be stacked atop one another by position_stack(). If you want them to be dodged side-to-side, use position_dodge() or position_dodge2(). Finally, position_fill() shows relative proportions at each x by stacking the bars and then standardising each bar to have the same height. Orientation This geom treats each axis differently and, thus, can thus have two orientations. Often the orientation is easy to deduce from a combination of the given mappings and the types of positional scales in use. Thus, ggplot2 will by default try to guess which orientation the layer should have. Under rare circumstances, the orientation is ambiguous and guessing may fail. In that case the orientation can be specified directly using the orientation parameter, which can be either “x” or “y.” The value gives the axis that the geom should run along, “x” being the default orientation you would expect for the geom. Aesthetics geom_bar() understands the following aesthetics (required aesthetics are in bold): x or y ! alpha colour fill group linetype size geom_col() understands the following aesthetics (required aesthetics are in bold): x or y ! alpha colour fill group linetype size stat_count() understands the following aesthetics (required aesthetics are in bold): x or y ! group weight Computed variables count: number of points in bin prop: groupwise proportion We often already have a table with a distribution that we want to present as a barplot. Here is an example of such a table: data(murders) tab &lt;- murders %&gt;% count(region) %&gt;% mutate(proportion = n/sum(n)) tab # &gt; region n proportion # &gt; 1 Northeast 9 0.176 # &gt; 2 South 17 0.333 # &gt; 3 North Central 12 0.235 # &gt; 4 West 13 0.255 We no longer want geombar to count, but rather just plot a bar to the height provided by the proportion variable. For this we need to provide x (the categories) and y (the values) and use the stat=“identity” option. tab %&gt;% ggplot(aes(region, proportion)) + geom_bar(stat = &quot;identity&quot;) 7.3.4 Density diamonds %&gt;% ggplot(aes(carat)) + geom_density() Computes and draws kernel density estimate, which is a smoothed version of the histogram. This is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution. geom_density( mapping = NULL, data = NULL, stat = &quot;density&quot;, position = &quot;identity&quot;, ..., na.rm = FALSE, orientation = NA, show.legend = NA, inherit.aes = TRUE, outline.type = &quot;upper&quot; ) stat_density( mapping = NULL, data = NULL, geom = &quot;area&quot;, position = &quot;stack&quot;, ..., bw = &quot;nrd0&quot;, adjust = 1, kernel = &quot;gaussian&quot;, n = 512, trim = FALSE, na.rm = FALSE, orientation = NA, show.legend = NA, inherit.aes = TRUE ) 7.3.5 Boxplots heights_metric %&gt;% ggplot(aes(sex,height)) + geom_boxplot() When using a geom_jitter and you don’t want to double plot outliers, you can hide the geom_boxplots outliers by setting outlier.shape to NA. heights_metric %&gt;% ggplot(aes(sex,height)) + geom_boxplot(outlier.shape = NA) + geom_jitter(alpha = 0.25, width = 0.25) 7.3.6 Lines https://ggplot2.tidyverse.org/reference/geom\\_path.html 7.3.7 Vertical Lines This adds a vertical line with an x intercept of 1963 and a blue color for visual effects. geom_vline(xintercept=1963, col = &quot;blue&quot;) 7.4 Coordinates 7.4.1 Limits Limit the scale like this: coord_cartesian(xlim = c(70,85)) 7.4.2 Flipping Flips the coordinate system, so that x and y are switched. coord_flip() geom_bar() + coord_flip() 7.5 Tiles 7.6 Scales 7.6.1 Logarithmic scales This is an axis which plots values on an exponantially “growing” line and is great for highly skewed distributions. Here is a video to freshen up on log scales: https://www.youtube.com/watch?v=sBhEi4L91Sg First, our desired scales are in log-scale. This is not the default, so this change needs to be added through a scales layer. A quick look at the cheat sheet reveals the scale_x_continuous function lets us control the behavior of scales. We use them like this: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_continuous(trans = &quot;log10&quot;) + scale_y_continuous(trans = &quot;log10&quot;) Because we are in the log-scale now, the nudge must be made smaller. This particular transformation is so common that ggplot2 provides the specialized functions scale_x_log10 and scale_y_log10, which we can use to rewrite the code like this: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() 7.6.2 Scale label format Change scale label formats like this: scale_x_continuous(labels = scales::comma) Change numeric year scale brakes: scale_x_continuous(breaks = seq(1960, 2020, by = 10)) Link for axis formatting: Axis Formatting 7.7 Labels Good labels are critical for making your plots accessible to a wider audience. Always ensure the axis and legend labels display the full variable name. Use the plot title and subtitle to explain the main findings. It’s common to use the caption to provide information about the data source. tag can be used for adding identification tags to differentiate between multiple plots. # Simple description: ggtitle(label, subtitle = waiver()) xlab(label) ylab(label) # More informative description: labs( ..., title = waiver(), subtitle = waiver(), caption = waiver(), tag = waiver(), alt = waiver(), alt_insight = waiver() ) Details You can also set axis and legend labels in the individual scales (using the first argument, the name). If you’re changing other scale options, this is recommended. If a plot already has a title, subtitle, caption, etc., and you want to remove it, you can do so by setting the respective argument to NULL. For example, if plot p has a subtitle, then p + labs(subtitle = NULL) will remove the subtitle from the plot. Change color or size legend label: labs(color = &quot;Country&quot;, size = &quot;GDP&quot;) "],["statistics.html", "8 Statistics 8.1 About Statistics 8.2 Centrality 8.3 Variability 8.4 Sampling", " 8 Statistics A descriptive statistic is a summary statistic that quantitatively describes or summarizes features from a collection of information, while descriptive statistics is the process of using and analyzing those statistics. Some measures that are commonly used to describe a data set are measures of central tendency and measures of variability or dispersion. Measures of central tendency include the mean, median and mode, while measures of variability include the standard deviation (or variance), the minimum and maximum values of the variables, kurtosis and skewness. The shape of the distribution may also be described via indices such as skewness and kurtosis. Characteristics of a variable’s distribution may also be depicted in graphical or tabular format, including histograms and stem-and-leaf display. 8.1 About Statistics Statistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as “all people living in a country” or “every atom composing a crystal.” Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments. Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution’s central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena. 8.2 Centrality Example Here are two different distributions. x &lt;- c(2,3,3,4,4,4,4,5,5,6) y &lt;- c(2,3,3,4,4,4,4,5,5,22) x is fairly normally distributed and y is highly skewed to the right: 8.2.1 Mean The mean is highly sensitive to skewed distributions. The single value of 22 in y instead of 6 in x changes the mean by 1.6. Therefore the median makes more sense as a measure centrality for y. mean(x) ## [1] 4 mean(y) ## [1] 5.6 8.2.2 Reproduction Mean and Standard Deviation Reproduction Example vector. vec = c(1, 4, 7, 9, 11) Calculate mean without mean() mean = sum(vec) / length(vec) mean ## [1] 6.4 mean(vec) # Correct ## [1] 6.4 Calculate variance without sd() variance = sum((vec - mean) ^ 2) / (length(vec) - 1) variance ## [1] 15.8 var(vec) # Correct ## [1] 15.8 Standard deviation without sd() st_dv = sqrt(variance) st_dv ## [1] 3.974921 sd(vec) # Correct ## [1] 3.974921 Calculate table of vectors with apply() and own sd function. my_sd_func = function(vector){ mean = mean(vector) variance = sum((vector - mean) ^ 2) / (length(vector) - 1) sqrt(variance) } # Test my_sd_func(vec) ## [1] 3.974921 # Test vectors test_vectors = rbind(c(2, 2, 3, 3, 4), c(7, 7, 8, 9, 9), c(2, 3, 5, 7, 8), c(1, 4, 7, 9, 11)) Apply own sd function on all test_vectors. apply(test_vectors, 1, my_sd_func) ## [1] 0.836660 1.000000 2.549510 3.974921 apply(test_vectors, 1, sd) ## [1] 0.836660 1.000000 2.549510 3.974921 8.2.3 Median The median is exactly the same for x and y. The median doesn’t take the value of 22 itself into account, but simply acknowledges the element position in an ordered array of y. median(x) ## [1] 4 median(y) ## [1] 4 8.2.4 Mode The mode is the same for both objects, because the value 4 appears most often. There doesn’t seem to be a mode() function, but this alternative works fine enough. which.max(table(x)) returns 1.) the biggest value of the tabled arrays and 2.) the index/location in the tabled array. which.max(table(x)) ## 4 ## 3 which.max(table(y)) ## 4 ## 3 With x and y its mode is both 4 and its position in the tabled array is 3. 8.3 Variability 8.3.1 Range Show the length of a vector. x &lt;- 1:10 length(x) ## [1] 10 Show the length of a matrix, i.e. the number of all cells. x &lt;- rbind(c(1:20), c(1:20)) length(x) ## [1] 40 The length of a data frame via length() returns only the number of columns, not the number of all cells. x &lt;- data.frame(c(1:10), rep(&quot;A&quot;, 10)) length(x) ## [1] 2 8.3.2 Range, IQR and SD x &lt;- c(5,4,3,6,7,3,5,7,4,4) y &lt;- c(0,0,4,8,7,9,9,7,3,1) z &lt;- c(5,4,3,6,7,3,5,7,4,4,1,5,6,7,3,8,8,3,40) Quick overview with summary() summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.00 4.00 4.50 4.80 5.75 7.00 Range range() shows the difference between the biggest and smallest value in a vector. To show the largest and smallest values: range(x) ## [1] 3 7 range(z) ## [1] 1 40 You need to calculate the actual range with this operation. max(x) - min(x) ## [1] 4 max(z) - min(z) ## [1] 39 Interquartile range The IQR is less affected by outliers. It shows the difference between the upper quartile and the lower quartile, i.e. the interquartile range. The middle 50%, i.e. the values between 25% to 75%. The difference can be big even without big outliers: max(x) - min(x) ## [1] 4 IQR(x) ## [1] 1.75 Note: IQR() is the same as: quantile(x, .75) - quantile(x, .25) ## 75% ## 1.75 Wow, what a difference with only one single big outlier: max(z) - min(z) ## [1] 39 IQR(z) ## [1] 3.5 8.4 Sampling In statistics, quality assurance, and survey methodology, sampling is the selection of a subset (a statistical sample) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt for the samples to represent the population in question. Two advantages of sampling are lower cost and faster data collection than measuring the entire population. 8.4.1 Weights Each observation measures one or more properties (such as weight, location, color) of observable bodies distinguished as independent objects or individuals. In survey sampling, weights can be applied to the data to adjust for the sample design, particularly in stratified sampling. Results from probability theory and statistical theory are employed to guide the practice. In business and medical research, sampling is widely used for gathering information about a population.  8.4.2 Population Successful statistical practice is based on focused problem definition. In sampling, this includes defining the “population” from which our sample is drawn. A population can be defined as including all people or items with the characteristic one wishes to understand. Because there is very rarely enough time or money to gather information from everyone or everything in a population, the goal becomes finding a representative sample (or subset) of that population. 8.4.3 Sampling In the most straightforward case, such as the sampling of a batch of material from production (acceptance sampling by lots), it would be most desirable to identify and measure every single item in the population and to include any one of them in our sample. However, in the more general case this is not usually possible or practical. There is no way to identify all rats in the set of all rats. Where voting is not compulsory, there is no way to identify which people will vote at a forthcoming election (in advance of the election). These imprecise populations are not amenable to sampling in any of the ways below and to which we could apply statistical theory. As a remedy, we seek a sampling frame which has the property that we can identify every single element and include any in our sample. The most straightforward type of frame is a list of elements of the population (preferably the entire population) with appropriate contact information. For example, in an opinion poll, possible sampling frames include an electoral register and a telephone directory. A probability sample is a sample in which every unit in the population has a chance (greater than zero) of being selected in the sample, and this probability can be accurately determined. The combination of these traits makes it possible to produce unbiased estimates of population totals, by weighting sampled units according to their probability of selection. Example: We want to estimate the total income of adults living in a given street. We visit each household in that street, identify all adults living there, and randomly select one adult from each household. (For example, we can allocate each person a random number, generated from a uniform distribution between 0 and 1, and select the person with the highest number in each household). We then interview the selected person and find their income. People living on their own are certain to be selected, so we simply add their income to our estimate of the total. But a person living in a household of two adults has only a one-in-two chance of selection. To reflect this, when we come to such a household, we would count the selected person’s income twice towards the total. (The person who is selected from that household can be loosely viewed as also representing the person who isn’t selected.) In the above example, not everybody has the same probability of selection; what makes it a probability sample is the fact that each person’s probability is known. When every element in the population does have the same probability of selection, this is known as an ‘equal probability of selection’ (EPS) design. Such designs are also referred to as ‘self-weighting’ because all sampled units are given the same weight. Probability sampling includes: Simple Random Sampling, Systematic Sampling, Stratified Sampling, Probability Proportional to Size Sampling, and Cluster or Multistage Sampling. These various ways of probability sampling have two things in common: Every element has a known nonzero probability of being sampled and involves random selection at some point. 8.4.4 Non-probability Sampling Non-probability sampling is any sampling method where some elements of the population have no chance of selection (these are sometimes referred to as ‘out of coverage’/‘undercovered’), or where the probability of selection can’t be accurately determined. It involves the selection of elements based on assumptions regarding the population of interest, which forms the criteria for selection. Hence, because the selection of elements is nonrandom, non-probability sampling does not allow the estimation of sampling errors. These conditions give rise to exclusion bias, placing limits on how much information a sample can provide about the population. Information about the relationship between sample and population is limited, making it difficult to extrapolate from the sample to the population. Example: We visit every household in a given street, and interview the first person to answer the door. In any household with more than one occupant, this is a non-probability sample, because some people are more likely to answer the door (e.g. an unemployed person who spends most of their time at home is more likely to answer than an employed housemate who might be at work when the interviewer calls) and it’s not practical to calculate these probabilities. Non-probability sampling methods include convenience sampling, quota sampling and purposive sampling. In addition, nonresponse effects may turn any probability design into a non-probability design if the characteristics of nonresponse are not well understood, since nonresponse effectively modifies each element’s probability of being sampled. 8.4.5 Sampling Methods Within any of the types of frames identified above, a variety of sampling methods can be employed, individually or in combination. Factors commonly influencing the choice between these designs include: Nature and quality of the frame Availability of auxiliary information about units on the frame Accuracy requirements, and the need to measure accuracy Whether detailed analysis of the sample is expected Cost/operational concerns 8.4.5.1 Simple random sampling In a simple random sample (SRS) of a given size, all subsets of a sampling frame have an equal probability of being selected. Each element of the frame thus has an equal probability of selection: the frame is not subdivided or partitioned. Furthermore, any given pair of elements has the same chance of selection as any other such pair (and similarly for triples, and so on). This minimizes bias and simplifies analysis of results. In particular, the variance between individual results within the sample is a good indicator of variance in the overall population, which makes it relatively easy to estimate the accuracy of results. Simple random sampling can be vulnerable to sampling error because the randomness of the selection may result in a sample that doesn’t reflect the makeup of the population. For instance, a simple random sample of ten people from a given country will on average produce five men and five women, but any given trial is likely to overrepresent one sex and underrepresent the other. Systematic and stratified techniques attempt to overcome this problem by “using information about the population” to choose a more “representative” sample. Also, simple random sampling can be cumbersome and tedious when sampling from a large target population. In some cases, investigators are interested in research questions specific to subgroups of the population. For example, researchers might be interested in examining whether cognitive ability as a predictor of job performance is equally applicable across racial groups. Simple random sampling cannot accommodate the needs of researchers in this situation, because it does not provide subsamples of the population, and other sampling strategies, such as stratified sampling, can be used instead. 8.4.5.2 Systematic sampling Systematic sampling (also known as interval sampling) relies on arranging the study population according to some ordering scheme and then selecting elements at regular intervals through that ordered list. Systematic sampling involves a random start and then proceeds with the selection of every kth element from then onwards. In this case, k=(population size/sample size). It is important that the starting point is not automatically the first in the list, but is instead randomly chosen from within the first to the kth element in the list. A simple example would be to select every 10th name from the telephone directory (an ‘every 10th’ sample, also referred to as ‘sampling with a skip of 10’). A visual representation of selecting a random sample using the systematic sampling technique As long as the starting point is randomized, systematic sampling is a type of probability sampling. It is easy to implement and the stratification induced can make it efficient, if the variable by which the list is ordered is correlated with the variable of interest. ‘Every 10th’ sampling is especially useful for efficient sampling from databases. For example, suppose we wish to sample people from a long street that starts in a poor area (house No. 1) and ends in an expensive district (house No. 1000). A simple random selection of addresses from this street could easily end up with too many from the high end and too few from the low end (or vice versa), leading to an unrepresentative sample. Selecting (e.g.) every 10th street number along the street ensures that the sample is spread evenly along the length of the street, representing all of these districts. (Note that if we always start at house #1 and end at #991, the sample is slightly biased towards the low end; by randomly selecting the start between #1 and #10, this bias is eliminated. However, systematic sampling is especially vulnerable to periodicities in the list. If periodicity is present and the period is a multiple or factor of the interval used, the sample is especially likely to be unrepresentative of the overall population, making the scheme less accurate than simple random sampling. For example, consider a street where the odd-numbered houses are all on the north (expensive) side of the road, and the even-numbered houses are all on the south (cheap) side. Under the sampling scheme given above, it is impossible to get a representative sample; either the houses sampled will all be from the odd-numbered, expensive side, or they will all be from the even-numbered, cheap side, unless the researcher has previous knowledge of this bias and avoids it by a using a skip which ensures jumping between the two sides (any odd-numbered skip). Another drawback of systematic sampling is that even in scenarios where it is more accurate than SRS, its theoretical properties make it difficult to quantify that accuracy. (In the two examples of systematic sampling that are given above, much of the potential sampling error is due to variation between neighbouring houses – but because this method never selects two neighbouring houses, the sample will not give us any information on that variation.) As described above, systematic sampling is an EPS method, because all elements have the same probability of selection (in the example given, one in ten). It is not ‘simple random sampling’ because different subsets of the same size have different selection probabilities – e.g. the set {4,14,24,…,994} has a one-in-ten probability of selection, but the set {4,13,24,34,…} has zero probability of selection. Systematic sampling can also be adapted to a non-EPS approach; for an example, see discussion of PPS samples below. 8.4.5.3 Stratified sampling When the population embraces a number of distinct categories, the frame can be organized by these categories into separate “strata.” Each stratum is then sampled as an independent sub-population, out of which individual elements can be randomly selected.The ratio of the size of this random selection (or sample) to the size of the population is called a sampling fraction. There are several potential benefits to stratified sampling. A visual representation of selecting a random sample using the stratified sampling technique First, dividing the population into distinct, independent strata can enable researchers to draw inferences about specific subgroups that may be lost in a more generalized random sample. Second, utilizing a stratified sampling method can lead to more efficient statistical estimates (provided that strata are selected based upon relevance to the criterion in question, instead of availability of the samples). Even if a stratified sampling approach does not lead to increased statistical efficiency, such a tactic will not result in less efficiency than would simple random sampling, provided that each stratum is proportional to the group’s size in the population. Third, it is sometimes the case that data are more readily available for individual, pre-existing strata within a population than for the overall population; in such cases, using a stratified sampling approach may be more convenient than aggregating data across groups (though this may potentially be at odds with the previously noted importance of utilizing criterion-relevant strata). Finally, since each stratum is treated as an independent population, different sampling approaches can be applied to different strata, potentially enabling researchers to use the approach best suited (or most cost-effective) for each identified subgroup within the population. There are, however, some potential drawbacks to using stratified sampling. First, identifying strata and implementing such an approach can increase the cost and complexity of sample selection, as well as leading to increased complexity of population estimates. Second, when examining multiple criteria, stratifying variables may be related to some, but not to others, further complicating the design, and potentially reducing the utility of the strata. Finally, in some cases (such as designs with a large number of strata, or those with a specified minimum sample size per group), stratified sampling can potentially require a larger sample than would other methods (although in most cases, the required sample size would be no larger than would be required for simple random sampling). A stratified sampling approach is most effective when three conditions are met Variability within strata are minimized Variability between strata are maximized The variables upon which the population is stratified are strongly correlated with the desired dependent variable. Advantages over other sampling methods Focuses on important subpopulations and ignores irrelevant ones. Allows use of different sampling techniques for different subpopulations. Improves the accuracy/efficiency of estimation. Permits greater balancing of statistical power of tests of differences between strata by sampling equal numbers from strata varying widely in size. Disadvantages Requires selection of relevant stratification variables which can be difficult. Is not useful when there are no homogeneous subgroups. Can be expensive to implement. Poststratification Stratification is sometimes introduced after the sampling phase in a process called “poststratification.” This approach is typically implemented due to a lack of prior knowledge of an appropriate stratifying variable or when the experimenter lacks the necessary information to create a stratifying variable during the sampling phase. Although the method is susceptible to the pitfalls of post hoc approaches, it can provide several benefits in the right situation. Implementation usually follows a simple random sample. In addition to allowing for stratification on an ancillary variable, poststratification can be used to implement weighting, which can improve the precision of a sample’s estimates. Oversampling Choice-based sampling is one of the stratified sampling strategies. In choice-based sampling, the data are stratified on the target and a sample is taken from each stratum so that the rare target class will be more represented in the sample. The model is then built on this biased sample. The effects of the input variables on the target are often estimated with more precision with the choice-based sample even when a smaller overall sample size is taken, compared to a random sample. The results usually must be adjusted to correct for the oversampling. 8.4.5.4 Cluster sampling Sometimes it is more cost-effective to select respondents in groups (‘clusters’). Sampling is often clustered by geography, or by time periods. (Nearly all samples are in some sense ‘clustered’ in time – although this is rarely taken into account in the analysis.) For instance, if surveying households within a city, we might choose to select 100 city blocks and then interview every household within the selected blocks. A visual representation of selecting a random sample using the cluster sampling technique Clustering can reduce travel and administrative costs. In the example above, an interviewer can make a single trip to visit several households in one block, rather than having to drive to a different block for each household. It also means that one does not need a sampling frame listing all elements in the target population. Instead, clusters can be chosen from a cluster-level frame, with an element-level frame created only for the selected clusters. In the example above, the sample only requires a block-level city map for initial selections, and then a household-level map of the 100 selected blocks, rather than a household-level map of the whole city. Cluster sampling (also known as clustered sampling) generally increases the variability of sample estimates above that of simple random sampling, depending on how the clusters differ between one another as compared to the within-cluster variation. For this reason, cluster sampling requires a larger sample than SRS to achieve the same level of accuracy – but cost savings from clustering might still make this a cheaper option. Cluster sampling is commonly implemented as multistage sampling. This is a complex form of cluster sampling in which two or more levels of units are embedded one in the other. The first stage consists of constructing the clusters that will be used to sample from. In the second stage, a sample of primary units is randomly selected from each cluster (rather than using all units contained in all selected clusters). In following stages, in each of those selected clusters, additional samples of units are selected, and so on. All ultimate units (individuals, for instance) selected at the last step of this procedure are then surveyed. This technique, thus, is essentially the process of taking random subsamples of preceding random samples. Multistage sampling can substantially reduce sampling costs, where the complete population list would need to be constructed (before other sampling methods could be applied). By eliminating the work involved in describing clusters that are not selected, multistage sampling can reduce the large costs associated with traditional cluster sampling.\\[8\\] However, each sample may not be a full representative of the whole population. 8.4.6 Errors and biases Survey results are typically subject to some error. Total errors can be classified into sampling errors and non-sampling errors. The term “error” here includes systematic biases as well as random errors. 8.4.6.1 Sampling errors and biases Sampling errors and biases are induced by the sample design. They include: Selection bias: When the true selection probabilities differ from those assumed in calculating the results. Random sampling error: Random variation in the results due to the elements in the sample being selected at random. 8.4.6.2 Non-sampling error Non-sampling errors are other errors which can impact final survey estimates, caused by problems in data collection, processing, or sample design. Such errors may include: Over-coverage: inclusion of data from outside of the population Under-coverage: sampling frame does not include elements in the population. Measurement error: e.g. when respondents misunderstand a question, or find it difficult to answer Processing error: mistakes in data coding Non-response or Participation bias: failure to obtain complete data from all selected individuals After sampling, a review should be held of the exact process followed in sampling, rather than that intended, in order to study any effects that any divergences might have on subsequent analysis. A particular problem involves non-response. Two major types of non-response exist: unit nonresponse (lack of completion of any part of the survey) item non-response (submission or participation in survey but failing to complete one or more components/questions of the survey) In survey sampling, many of the individuals identified as part of the sample may be unwilling to participate, not have the time to participate (opportunity cost), or survey administrators may not have been able to contact them. In this case, there is a risk of differences between respondents and nonrespondents, leading to biased estimates of population parameters. This is often addressed by improving survey design, offering incentives, and conducting follow-up studies which make a repeated attempt to contact the unresponsive and to characterize their similarities and differences with the rest of the frame. The effects can also be mitigated by weighting the data (when population benchmarks are available) or by imputing data based on answers to other questions. Nonresponse is particularly a problem in internet sampling. Reasons for this problem may include improperly designed surveys, over-surveying (or survey fatigue, and the fact that potential participants may have multiple e-mail addresses, which they don’t use anymore or don’t check regularly. "],["probability-thoery.html", "9 Probability Thoery 9.1 Conditional Probability 9.2 Permutations 9.3 Combinations 9.4 Random Variables 9.5 Normal Distribution 9.6 Binomial Distribution 9.7 Bernoulli Distribution 9.8 Geometric Distribution", " 9 Probability Thoery The reason we study statistics and probability together is because when we collect data as part of a statistical study, we want to be able to use what we know about probability to say how likely it is that our results are reliable. Probability is how likely it is that something will occur. You can write a probability as a fraction, decimal or percent, but all probabilities are numbers equal to or between 0 and 1. The formula: \\[ P(event) = \\frac{\\text{outcomes that meet our criteria}}{\\text{all possible outcomes (also: sample space)}} \\] Example: How likely is it to draw a queen from a deck of playing cards? First, how many outcomes or cards meet our criteria? Four. How many possible outcomes are there? A card deck has 52 cards. \\[ P(queen) = \\frac{4}{52}=0.07692307692≈7.7 \\% \\] For example, let’s say we flip a coin four times in a row. As you know, it’s completely possible that, just by chance, we end up with four heads in a row. Based on that result, we might say that the probability of getting heads is \\[ P(heads) = \\frac{4}{4}= 1 = 100 \\% \\] But how can this be true? Before we saw that the probability of getting heads on one flip was 50 % , but now we’re calculating the probability of getting heads four times in a row at 100 % . What’s going on? We’re looking at the difference between experimental and theoretical probability. Experimental probability (also called empirical probability) is the probability we find when we run experiments, basically the results of an experiment. If we flip the coin a fifth time and get tails this time, then the experimental probability of getting heads after 5 experiments is \\[ P(heads) = \\frac{4}{5}= 80\\% \\] In other words, the experimental probability of an event will be constantly changing as we run more and more experiments over time. If the experiment is a good one, the idea is that over time the experimental probability will get very close to the theoretical probability. Theoretical probability (also called classical probability) is the probability that an event will occur if you could run an infinite number of experiments. Or, you can think about the theoretical probability as the one we get from the simple probability formula: \\[ P(event) = \\frac{\\text{outcomes that meet our criteria}}{\\text{all possible outcomes}} \\] We know from using this formula that the probability of getting heads when we flip a coin is 50 % . Therefore, the theoretical probability is 50 % , which means that the more experiments we run, the closer our experimental probability should get to 50 % . This is also called the law of large numbers. It says that, if we could run an infinite number of experiments, that our experimental probability would eventually equal our theoretical probability. 9.1 Conditional Probability Bayes’ theorem or condtional probability describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if the risk of developing health problems is known to increase with age, Bayes’ theorem allows the risk to an individual of a known age to be assessed more accurately (by conditioning it on their age) than simply assuming that the individual is typical of the population as a whole. Bayes’ theorem is also called Bayes’ Rule or Bayes’ Law and is the foundation of the field of Bayesian statistics. Bayes’ theorem is stated mathematically as the following equation: \\[ P(A|B)=\\frac{P(B|A)*P(A)}{P(B)} \\] Which tells us: how often \\(A\\) happens given that \\(B\\) happens, written \\(P(A|B)\\). When we know: how often \\(B\\) happens given that \\(A\\) happens, written \\(P(B|A)\\). how likely \\(A\\) is on its own, written \\(P(A)\\). how likely \\(B\\) is on its own, written \\(P(B)\\). Example Let us say P(Fire) means how often there is fire, and P(Smoke) means how often we see smoke, then: P(Fire|Smoke) means how often there is fire when we can see smoke  P(Smoke|Fire) means how often we can see smoke when there is fire So the formula kind of tells us “forwards” P(Fire|Smoke) when we know “backwards” P(Smoke|Fire) dangerous fires are rare (1%), \\(P(Fire)\\). but smoke is fairly common (10%) due to barbecues, \\(P(Smoke)\\). and 90% of dangerous fires make smoke, smoke given dangerous fire, \\(P(Smoke|Fire)\\). We can then discover the probability of dangerous Fire when there is (given) Smoke: \\[ P(Fire|Smoke) = \\frac{P(Smoke|Fire)*P(Fire)}{P(Smoke)}=\\frac{90\\%*1\\%}{10\\%}= 9\\% \\] So it is still worth checking out any smoke to be sure. 9.2 Permutations Permutations and Combinations are the number of ways you can arrange or combine a set of things. Permutations are used when the order does matter, meaning 123 is not the same as 213. In combinations the order does not matter. This is often used for questions like what’s the probability to get at least 5 coin flips with head in 10 total tosses. I.e., the order of the heads in when tossing is not important, the total amount is. There are two types of permutations (remember the order does matter). 9.2.1 Permutations with repetition Example: a digit lock (it could be 333). How many different ways are there to input a 3-digit lock with numbers from 0 to 9 are there? The formula: \\[ n^r \\] where \\(n\\) is the number of distinct things to choose from and we choose \\(r\\) of them. The answer: \\[ n^r=10^3=1000 \\] 9.2.2 Permutations without repetition Example 1): arrangement of 16 pool balls. You pick one ball and the next has one option less to choose from. How many different ways are there to arrange 16 distinct pool balls? The formula: \\[ P(n,r)=\\frac{n!}{(n-r)!} \\] Example 1) the answer: \\[ P(n,r)=\\frac{n!}{(n-r)!} = P(16,16) = \\frac{16!}{(16-16)!}=\\frac{16!}{1}=20.922.789.888.000 \\] When we calculate Permutations without repetition each time we have to subtract one less option to choose from, because we’ve already used one distinct value. I.e. \\(5*4*3\\) instead of \\(5*5*5\\). This process of decrementing a value by 1 repeatedly is mathematically notated by \\(n! = (5-1)*(4-1)*(3-1)*(2-1)\\). Because we only choose three (\\(r\\)) numbers of the set of 5 numbers we have to divide them by the excess permutations. Example 2) How many different three letter “words” can be formed by the letters abcde? Example 2) the answer: \\[ P(5,3)=\\frac{5!}{(5-3)!}=\\frac{5!}{2!}= \\frac{5*4*3*2*1}{2*1}= 5*4*3=60 \\] Same as: abc,abd,abe,acb,acd,ace,adb,adc,ade,aeb,aec,aed (12 with a first) bac,bad,bae,…. (so 12 times 5 = 60). 9.3 Combinations On the other hand, a Combination is the number of ways you can arrange a set of things, but the order doesn’t matter. There are also two types of combinations (remember the order does not matter now): 9.3.1 Combinations with repetitions Example: coins in your pocket (5,5,5,10,10) This is actually the most complicated and I haven’t had the need to use it so far. 9.3.2 Combinations without repetition Example: lottery numbers (2,14,15,27). How many combinations are there in choosing 5 lottery numbers from 1 to 10? The formula, also called the Binomial Coefficient: \\[ P(n,r)=\\frac{n!}{r!(n-r)!} \\] The answer: \\[ P(n,r)=\\frac{n!}{r!(n-r)!}=\\frac{10!}{5!(10-5)!}=\\frac{10!}{5!*5!}=252 \\] Same as: 1,2,3,4,5 (counted the same as 2,1,3,4,5 and 3,1,2,4,5 etc.) - 1,2,3,4,6 - … 9.4 Random Variables A variable can take at least two different values. For a random sample or randomized experiment, each possible outcome has a probability that it occurs. The variable itself is sometimes then referred to as a random variable. This terminology emphasizes that the outcome varies from observation to observation according to random variation that can be summarized by probabilities. Another way to think about the concept of random variables is as placeholders for otherwise long explanations of events. For example, the notation of the probability of getting two heads in an experiment of two coin flips could be written as \\(P(\\text{getting two heads in 2 flips})\\). But if you use a variable X, usually written large, that increases by \\(1\\) every time a flip shows a head you could write it as such: \\(P(X=2)\\). Much neater. 9.4.1 Discrete Random Variables A variable is discrete if the possible outcomes are a set of separate values, such as a variable expressed as “the number of …” with possible values 0, 1, 2, …. If you can count them. Example: Let y denote the response to the question “What do you think is the ideal number of children for a family to have?” This is a discrete variable, taking the possible values 0, 1, 2, 3, and so forth. According to recent General Social Surveys, for a randomly chosen person in the United States the probability distribution of Y is approximately as the table shows. The table displays the recorded y-values and their probabilities. For instance, P(4), the probability that Y = 4 children is regarded as ideal, equals 0.12. Each probability in table is between 0 and 1, and the sum of the probabilities equals 1. A histogram can portray the probability distribution. The rectangular bar over a possible value of the variable has height equal to the probability of that value. The bar over the value 4 has height 0.12, the probability of the outcome 4. 9.4.2 Continuous Random Variables It is continuous if the possible outcomes are an infinite continuum, such as all the real numbers between 0 and 1. If you can measure it. A probability distribution lists the possible outcomes and their probabilities. Example: Commuting Time to Work A recent U.S. Census Bureau study about commuting time for workers in the United States who commute to work 2 measured y = travel time, in minutes. The probability distribution of y provides probabilities such as P(y &lt; 15), the probability that travel time is less than 15 minutes, or P(30 &lt; y &lt; 60), the probability that travel time is between 30 and 60 minutes. This figure portrays the probability distribution of y. The shaded area in the figure refers to the region of values higher than 45. This area equals 15% of the total area under the curve, representing the probability of 0.15 that commuting time is more than 45 minutes. Those regions in which the curve has relatively high height have the values most likely to be observed. 9.4.3 Transforming Random Variables (Shift or Scale) When transforming a random variable the corresponding metrics change depending on weather the variable is being shifted, by adding or subtracting (+k or -k) the data, or scaled, by multiplying or dividing (*k or / k). Shifting The measures of centrality change by + or - k. The measures of variability don’t change. Scaling The measures of centrality change by * or / k. The measures of variability change by * or / k. 9.4.4 Expected Value (Mean) Like a population distribution, a probability distribution has parameters describing center and variability. The mean describes center and the standard deviation describes variability. The parameter values are the values these measures would assume, in the long run, if the randomized experiment or random sample repeatedly took observations on the variable y having that probability distribution. For example, suppose we take observations from the distribution in child number preferences. Over the long run, we expect y = 0 to occur 1% of the time, y = 1 to occur 3% of the time, and so forth. In 100 observations, for instance, we expect about: one 0, three 1 ′ s, sixty 2 ′ s, twenty-three 3 ′ s, twelve 4 ′ s, and one 5. In that case, since the mean equals the total of the observations divided by the sample size, the mean equals: \\[ \\frac{0(1) + 1(3) + 2(60) + 3(23) + 4(12) + 5(1)}{100}=\\frac{245}{100}=2.45 \\] The calculation has the form: \\[ 0(0.01)+ 1(0.03) + 2(0.60) + 3(0.23) + 4(0.12) + 5(0.01) \\] The mean of a probability distribution is also called the expected value. The terminology reflects that E(y) represents what we expect for the average value of y in a long series of observations. 9.4.5 Standard Deviation The standard deviation of a probability distribution, denoted by σ, measures its variability. The more spread out the distribution, the larger the value of σ. The Empirical Rule helps us to interpret σ. If a probability distribution is bell shaped, about 68% of the probability falls between μ−σ and μ + σ, about 95% falls between μ − 2σ and μ + 2σ, and all or nearly all falls between μ − 3σ and μ + 3σ. The standard deviation σ is the square root of the variance σ² of the probability distribution. The variance measures the average squared deviation of an observation from the mean. That is, it is the expected value of (y − μ)² . In the discrete case, the formula is \\[ σ^2= E(y-μ)^2= \\sum(y-μ)^2*P(y) \\] 9.5 Normal Distribution Early statisticians noticed the same shape coming up over and over again in different distributions, so they named it the normal distribution. In probability theory, a normal (or Gaussian or Gauss or Laplace-Gauss) distribution is a type of continuous probability distribution for a real-valued random variable. The parameter μ is the mean or expectation of the distribution (and also its median and mode), while the parameter σ is its standard deviation. The variance of the distribution is σ². Their importance is partly due to the central limit theorem. It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable - whose distribution converges to a normal distribution as the number of samples increases. 9.5.1 Properties Normal distributions have the following features: - symmetric bell shape - mean and median are equal, both located at the center of the distribution - ≈ 68% of the data falls within 1 standard deviation of the mean - ≈ 95% of the data falls within 2 standard deviation of the mean - ≈ 99.7% of the data falls within 3 standard deviation of the mean For example, heights of adult females in North America have approximately a normal distribution with μ = 65.0 inches and σ = 3.5. The probability is nearly 1.0 that a randomly selected female has height between μ − 3σ = 65.0 − 3(3.5) = 54.5 inches and μ + 3σ = 65.0 + 3(3.5) = 75.5 inches. Adult male height has a normal distribution with μ = 70.0 and σ = 4.0 inches. So, the probability is nearly 1.0 that a randomly selected male has height between μ − 3σ = 70.0 − 3(4.0) = 58 inches and μ + 3σ = 70.0 + 3(4.0) = 82 inches. 9.5.2 Z-Scores In statistics, the standard score is the number of standard deviations by which the value of a raw score (i.e., an observed value or data point) is above or below the mean value of what is being observed or measured. Raw scores above the mean have positive standard scores, while those below the mean have negative standard scores. It is calculated by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation. This process of converting a raw score into a standard score is called standardizing or normalizing (however, “normalizing” can refer to many types of ratios; see normalization for more). Standard scores are most commonly called z-scores; the two terms may be used interchangeably, as they are in this article. Other terms include z-values, normal scores, and standardized variables. Computing a z-score requires knowing the mean and standard deviation of the complete population to which a data point belongs; if one only has a sample of observations from the population, then the analogous computation with sample mean and sample standard deviation yields the t-statistic. A z-score measures exactly how many standard deviations above or below the mean a data point is. Here is the formula: \\[ z = \\frac{\\text{data point - mean}}{\\text{standard deviation}} \\] Here is the same formula written with symbols: \\[ z = \\frac{x-μ}{σ} \\] Here are some important facts about z-scores: A positive z-score says the data point is above average. A negative z-score says the data point is below average. A z-score close to 0 says the data point is close to average. A data point can be considered unusual if its z-score is above 3 or below − 3. Example The grades on a history midterm at Almond have a mean of μ = 85 and a standard deviation of σ = 2. Michael scored 86 on the exam. Find the z-score for Micheal’s exam grade. \\[ z = \\frac{\\text{his grade - mean grade}}{\\text{standard deviation}} \\] \\[ z = \\frac{86-85}{2}=\\frac{1}{2}=0.5 \\] Micheal’s z-score is 0.5 point. His grade was half of a standard deviation above the mean. 9.5.3 Examples 9.5.3.1 Finding Percentages Example: a certain variety of tree has a mean trunk diameter of μ = 150 cm and a standard deviation of σ = 30cm. Approximately what percent of these trees have a diameter greater than 210 cm? Solution: Step 1: Sketch a normal distribution with a mean of μ = 150 cm and a standard deviation of σ = 30 cm. Step 2: The diameter of 210 cm is two standard deviations above the mean. Shade above that point. Step 3: Add the percentages in the shaded area: \\[ 2.35+0.15 = 2.5\\text{%} \\] About 2.5% of these trees have a diameter greater than 210 cm. 9.5.3.2 Expected Value Example: a certain variety of pine tree has a mean trunk diameter of μ = 150 cm and a standard deviation of σ = 30 cm. A certain section of a forest has 500 of these trees. Approximately how many of these trees have a diameter smaller than 120 cm? Solution: Step 1: Sketch a normal distribution with a mean of μ = 150 cm and a deviation of σ = 30 cm. Step 2: The diameter of 120 cm is one standard deviation below the mean. Shade below that point. Step 3: Add the percentage in the shaded area: \\[ 0.15+2.35+13.5=16\\% \\] About 16 % of these trees have a diameter smaller than 120 cm. Step 4: Find how many trees in the forest that percent represents. We need to find how many trees 16 % of 500 is. \\[ 16\\% \\text{ of }500=0.16*500=80 \\] About 80 trees have a diameter smaller than 120 cm. 9.5.4 In R The pnorm() function in R accepts a z-value/standard deviation value and returns the cumulative probability below that value. The qnorm() function returns the z-value/standard deviation value from a probabilty. pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) 9.5.4.1 Calculate Probability From Z-Value If you have the standard deviation or z-value and want to find out the probability. Example 1 The height of middle school students is normally distributed with a mean of 150 cm and a standard deviation of 20 cm. What is the probability that randomly selected students have a height greater than 170 cm, denoted by the random variable X. In our example we need the cumulative probability above that value (the right side of the curve) and therefore need to subtract the function value from 1. 1-pnorm(1) ## [1] 0.1586553 Or even more elaborate using all parameters in the pnorm() function. pnorm(q = 170, mean = 150, sd = 20, lower.tail = F) ## [1] 0.1586553 Answer: the probability of student randomly selected with a height above 170 cm, i.e. above one standard deviation from the mean, is 15.9 %. Example 2 A set of middle school student heights are normally distributed with a mean of 150 cm and a standard deviation of 20 centimeters. Let X = the height of a randomly selected student from this set. Find out: \\(P(140 &lt; X &lt; 154)\\). Step 1: Find out the left tail probability for X below 154 cm: 0.58. Step 2: Find out the left tail probability for X below 140 cm: 0.31. Step 3: To find out the probability for in between X &lt; 154 and X &gt; 140 subtract the first from the latter. pnorm(q = 154, mean = 150, sd = 20, lower.tail = T) - pnorm(q = 140, mean = 150, sd = 20, lower.tail = T) ## [1] 0.2707222 Answer: the probability for a randomly selected student to have a height between 140 and 154 cm is 31 %. Find probabilities if you have a z-value For the left-tail/cumulative probabilities, i.e. mean - z x standard deviation, in a normal distribution. Calculate it by a specific z value. pnorm(2) # cumulative probability below mu + 2.0(sigma) ## [1] 0.9772499 pnorm(-1) # The P() of falling below the sd of -1. ## [1] 0.1586553 To find the right-tail probabilities for a specific z value in a normal distribution subtract it from 1. 1-pnorm(2) # The P() of falling over the sd of 2. ## [1] 0.02275013 1-pnorm(-1)# The P() of falling over the sd of -1. ## [1] 0.8413447 The probability of falling within 2 standard deviations of the mean: 1 - 2 * pnorm(-2) ## [1] 0.9544997 Show that 90% of a normal probability distribution falls between mean - 1.64 and mean + 1.64. 1 - 2 * (pnorm(1.64)) ## [1] -0.8989948 9.5.4.2 Calculate Z-Value From Probability If, one the other hand, you have the probability for an event and want to find out the z-value or standard deviation you can use the qnorm() function. qnorm() accepts a probability of a normal distribution and return the z-value associated with said probability. Notice that qnorm() also return the left side of the distribution. If you need the right side simply put a minus in front. qnorm(0.1586) ## [1] -1.000228 -qnorm(0.1586) ## [1] 1.000228 Find z-value if you have a probability To the the z-values corresponding to specific probability values we can use this. qnorm(0.975) # q denotes &quot;quantile&quot;; .975 quantile = 97.5 percentile ## [1] 1.959964 # The z-value is 1.96, rounded to two decimals More checks. qnorm(.05) ## [1] -1.644854 qnorm(0.01) ## [1] -2.326348 # To get the right-tail probability. -qnorm(0.005) ## [1] 2.575829 9.6 Binomial Distribution In probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes–no question, and each with its own Boolean-valued outcome: success (with probability p) or failure (with probability q = 1 − p). A single success/failure experiment is also called a Bernoulli trial or Bernoulli experiment, and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., n = 1, the binomial distribution is a Bernoulli distribution. The binomial distribution is the basis for the popular binomial test of statistical significance. The binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size N. If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one. However, for N much larger than n, the binomial distribution remains a good approximation, and is widely used. 9.6.1 Expected Value (Mean) and Variance If X ~ B(n, p), that is, X is a binomially distributed random variable, n being the total number of experiments and p the probability of each experiment yielding a successful result, then the expected value of X is: \\[ E(X)=np \\] This follows from the linearity of the expected value along with fact that X is the sum of n identical Bernoulli random variables, each with expected value p. In other words, if \\(X_1,...,X_n\\) are identical (and independent) Bernoulli random variables with parameter p, then \\(X=X_1+…+X_n\\) and \\(E(X)=E[X_1+...+X_n]=E(X_1)+...+E(X_n)=p+...p=np\\) The variance is: \\[ Var(X)=np(1-p) \\] This similarly follows from the fact that the variance of a sum of independent random variables is the sum of the variances. 9.6.2 In R Example Suppose there are twelve multiple choice questions in an English class quiz. Each question has five possible answers, and only one of them is correct. Find the probability of having four or less correct answers if a student attempts to answer every question at random. Answer Since only one out of five possible answers is correct, the probability of answering a question correctly by random is 1/5=0.2. We can find the probability of having exactly 4 correct answers by random attempts as follows. We use the dbinom() function for a single successful outcome, e.g. \\(r=4\\), and pbinom() for cumulative successful outcomes, e.g. \\(r&lt;=4\\). dbinom(x = 4, size = 12, prob = 0.2) ## [1] 0.1328756 The probability for 4 successful outcomes or less. pbinom(4, 12, 0.2) ## [1] 0.9274445 9.7 Bernoulli Distribution The Bernoulli distribution is a special case of the binomial distribution where a single trial is conducted (so n would be 1 for such a binomial distribution). It is also a special case of the two-point distribution, for which the possible outcomes need not be 0 and 1. In probability theory and statistics, the Bernoulli distribution is the discrete probability distribution of a random variable which takes the value \\(1\\) with probability \\(p\\) and the value \\(0\\) with probability \\(q = 1 −p\\). Less formally, it can be thought of as a model for the set of possible outcomes of any single experiment that asks a yes–no question. Such questions lead to outcomes that are boolean-valued: a single bit whose value is success/yes/true/one with probability p and failure/no/false/zero with probability \\(q\\). It can be used to represent a (possibly biased) coin toss where 1 and 0 would represent “heads” and “tails” (or vice versa), respectively, and p would be the probability of the coin landing on heads or tails, respectively. In particular, unfair coins would have \\(p ≠ 1/2\\). 9.7.1 Properties If \\(X\\) is a random variable with this distribution, then: \\[ Pr(X=1)=p=1-Pr(X=0)=1-q \\] The Bernoulli distribution is a special case of the binomial distribution with \\(n=1\\). 9.7.2 Mean The expected value of a Bernoulli random variable X is \\[ E(X) = p \\] This is due to the fact that for a Bernoulli distributed random variable \\(X\\) with \\(Pr(X=1)=p\\) and \\(Pr(X=0)=q\\) we find \\[ E(X)=Pr(X=1)*1+Pr(X=0)*0=p*1+q*0=p \\] 9.7.3 Variance The variance of a Bernoulli distributed X is \\[ Var(X)=pq=p(1-p) \\] We first find \\[ E(X^2)=Pr(X=1)*1^2+Pr(X=0)*0^2=p*1^2+q*0^2=E(X) \\] From this follows \\[ Var(X)=E(X^2)-E(X)^2=E(X)-E(X)^2=p-p^2=p(1-p)=pq \\] With this result it is easy to prove that, for any Bernoulli distribution, its variance will have a value between \\(0\\) and \\(1/4\\). 9.7.4 In R The required package for Bernoulli operations in R is called Rlab. Density, distribution function, quantile function and random generation for the Bernoulli distribution with parameter prob. Rlab::dbern(x = 3, prob = .5) ## [1] 0 9.8 Geometric Distribution In probability theory and statistics, the geometric distribution is either one of two discrete probability distributions: The probability distribution of the number X of Bernoulli trials needed to get one success, supported on the set { 1, 2, 3, … } The probability distribution of the number Y = X − 1 of failures before the first success, supported on the set { 0, 1, 2, 3, … } Which of these one calls “the” geometric distribution is a matter of convention and convenience. These two different geometric distributions should not be confused with each other. Often, the name shifted geometric distribution is adopted for the former one (distribution of the number X); however, to avoid ambiguity, it is considered wise to indicate which is intended, by mentioning the support explicitly. The geometric distribution gives the probability that the first occurrence of success requires k independent trials, each with success probability p. If the probability of success on each trial is p, then the probability that the kth trial (out of k trials) is the first success is \\[ Pr(X=k)=(1-p)^{k-1}p \\] for k = 1, 2, 3, …. The above form of the geometric distribution is used for modeling the number of trials up to and including the first success. By contrast, the following form of the geometric distribution is used for modeling the number of failures until the first success: \\[ Pr(Y=k)=Pr(X=k+1)=(1-p)^kp \\] for k = 0,1,2,3, … In either case, the sequence of probabilities is a geometric sequence. For example, suppose an ordinary die is thrown repeatedly until the first time a “1” appears. The probability distribution of the number of times it is thrown is supported on the infinite set { 1, 2, 3, … } and is a geometric distribution with p = 1/6. The geometric distribution is denoted by Geo(p) where 0 &lt; p ≤ 1. Consider a sequence of trials, where each trial has only two possible outcomes (designated failure and success). The probability of success is assumed to be the same for each trial. In such a sequence of trials, the geometric distribution is useful to model the number of failures before the first success. The distribution gives the probability that there are zero failures before the first success, one failure before the first success, two failures before the first success, and so on. When is the geometric distribution an appropriate model? The geometric distribution is an appropriate model if the following assumptions are true. The phenomenon being modeled is a sequence of independent trials. There are only two possible outcomes for each trial, often designated success or failure. The probability of success, p, is the same for every trial. If these conditions are true, then the geometric random variable Y is the count of the number of failures before the first success. The possible number of failures before the first success is 0, 1, 2, 3, and so on. In the graphs above, this formulation is shown on the right. An alternative formulation is that the geometric random variable X is the total number of trials up to and including the first success, and the number of failures is X − 1. In the graphs above, this formulation is shown on the left. 9.8.1 In R Not sure if those are all correct! R function dgeom() calculates the probability of x failures prior to the first success. Example: I pick cards from a standard deck until I get a king (I replace the cards if they are not a king). What is the probability that I need to pick 5 cards, i.e. I pick 4 unsuccessful cards (x=4) and the fifth is a success. dgeom(x= 4,prob = 1/13) ## [1] 0.05584808 The function pgeom() is the cumulative probability of less than or equal to q failures prior to success. Example 1: What is the probability that I pick less than 10 cards? pgeom(q = 9, prob = 1/13, lower.tail = F) ## [1] 0.4491371 Example 2: What is the probability that I need to pick more than 12 cards? 1-pgeom(q = 13, prob = 1/13, lower.tail = F) ## [1] 0.6739152 "],["inferential-statistics.html", "10 Inferential Statistics 10.1 Sampling Distributions 10.2 Central Limit Theorem 10.3 Confidence Intervals 10.4 Hypothesis Testing 10.5 Correlation 10.6 Regression", " 10 Inferential Statistics To draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented while accounting for randomness. These inferences may take the form of answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation), and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction, and estimation of unobserved values either in or associated with the population being studied. It can include extrapolation and interpolation of time series or spatial data, and data mining. 10.1 Sampling Distributions A sampling distribution refers to a probability distribution of a statistic that comes from choosing random samples of a given population. Also known as a finite-sample distribution, it represents the distribution of frequencies on how spread apart various outcomes will be for a specific population. 1. Sampling distribution of mean As shown from the example above, you can calculate the mean of every sample group chosen from the population and plot out all the data points. The graph will show a normal distribution, and the center will be the mean of the sampling distribution, which is the mean of the entire population. 2. Sampling distribution of proportion It gives you information about proportions in a population. You would select samples from the population and get the sample proportion. The mean of all the sample proportions that you calculate from each sample group would become the proportion of the entire population. 3. T-distribution T-distribution is used when the sample size is very small or not much is known about the population. It is used to estimate the mean of the population, confidence intervals, statistical differences, and linear regression. 10.2 Central Limit Theorem The Central Limit Theorem (CLT) tells us that when the number of draws, also called the sample size, is large, the probability distribution of the sum of the independent draws is approximately normal. Because sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history. 10.2.1 What is Large Enough for the CTL? The CLT works when the number of draws is large. But large is a relative term. In many circumstances as few as 30 draws is enough to make the CLT useful. In some specific instances, as few as 10 is enough. However, these should not be considered general rules. Note, for example, that when the probability of success is very small, we need much larger sample sizes. By way of illustration, let’s consider the lottery. In the lottery, the chances of winning are less than 1 in a million. Thousands of people play so the number of draws is very large. Yet the number of winners, the sum of the draws, range between 0 and 4. This sum is certainly not well approximated by a normal distribution, so the CLT does not apply, even with the very large sample size. This is generally true when the probability of a success is very low. In these cases, the Poisson distribution is more appropriate. 10.2.2 Proof of Central Limit Theorem Let’s create a sampling model and calculate a random statistic for the proportion p. n = 100 p = 0.4 x = sample(c(1,0), size = n, replace = T, prob = c(p, (1-p))) x_bar= mean(x) x_bar ## [1] 0.38 Now let’s create a Monte Carlo simulation in which we calculate the sample statistic p 10,000 times. B = 10000 x_bar_distribution = replicate(B, { x = sample(c(1,0), size = n, replace = T, prob = c(p, (1-p))) mean(x) }) head(x_bar_distribution) ## [1] 0.34 0.37 0.41 0.44 0.42 0.38 The Central Limit Theorem tells us that when the number of draws, also called the sample size, is large, the probability distribution of the sum (or mean/proportion) of the independent draws is approximately normal. Let’s test that by plotting a histogram and checking with qqnorm(). hist(x_bar_distribution) # Looks good qqnorm(x_bar_distribution);qqline(x_bar_distribution) # Looks good enoguh. What’s the practical use of this proof? Because of it we can now use the normal distribution function to calculate probabilities. What is the probability that we get a proportion that is smaller than ? We first need the calculate the standard error of our sample. SE = sqrt(p * (1-p) / n) 3 * SE ## [1] 0.1469694 The sampling distribution of the sample proportion is approximately normal and normal distributions encompass almost all possible values within 3 standard deviations from the mean. We can therefore say that 99.7 % of all samples proportion statistics for n = 100 and p = 0.4 fall within +- 0.147 of 0.4. Here is the proof. We can compare the results from Monte Carlo simulation sampling distribution with standard normal distribution function. I.e. what’s the proportions/probability of falling outside of 3 standard deviations from the mean? mean(x_bar_distribution &lt; p - 3 * SE) + mean(x_bar_distribution &gt; p + 3 * SE) ## [1] 0.0029 pnorm(-3) + 1 - pnorm(3) ## [1] 0.002699796 This does in fact check out! The proportions from the Monte Carlo simulation and the probabilities from the normal distribution function do approximate each other very well. The applications based on these insights alone are somewhat limited. In real life we rarely have the probabilities for the population parameters. The power does show itself however when making inferences from one single sample, its proportion statistic and its standard deviation, about the population parameter. This is because we now know (experimentally proven) that the sampling distribution of the sample proportion is normally distributed. 10.3 Confidence Intervals A confidence interval (abbreviated CI) is used for the purpose of estimating a population parameter (a single number that describes a population) by using statistics (numbers that describe a sample of data). For example, you might estimate the average household income (parameter) based on the average household income from a random sample of 1,000 homes (statistic). However, because sample results will vary, you need to add a measure of that variability to your estimate. This measure of variability is called the margin of error, the heart of a confidence interval. Your sample statistic, plus or minus your margin of error, gives you a range of likely values for the parameter — in other words, a confidence interval. How do we do it? To estimate a parameter with a confidence interval: Choose your confidence level and your sample size. Select a random sample of individuals from the population. Collect reliable and relevant data from the individuals in the sample. Summarize the data into a statistic (for example, a sample mean or proportion). Calculate the margin of error. \\(\\pm z^{*} \\frac{\\sigma}{\\sqrt{n}}\\) Take the statistic plus or minus the margin of error to get your final estimate of the parameter: \\(\\bar{x} \\pm z^{*} \\frac{\\sigma}{\\sqrt{n}}\\) 10.3.1 Why Confidence Intervals Work What do we know so far and how does it help with confidence intervals? CLT: If \\(n\\) is large enough, the sampling distribution of sample statistics (mean, proportion) approximates a normal distribution -&gt; we can use Z-Scores or T-Scores to calculate probabilities. The mean/proportion of the sampling distribution of the sample means/proportions is equal to the true population mean/proportion \\(\\mu_\\hat{x} = \\mu\\) or \\(p_\\hat{x} = p\\) -&gt; \\[ \\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}} \\] Since the sample proportion \\(\\hat{\\pi}\\) is a sample mean, the Central Limit Theorem applies: For large random samples, the sampling distribution of \\(\\hat{\\pi}\\) is approximately normal about the parameter \\(\\pi\\) it estimates. This means that each sample proportion, in the graphic below annotated with \\(\\hat{\\pi}\\), has a The confidence interval is the range of values that you expect your estimate to fall between a certain percentage of the time if you run your experiment again or re-sample the population in the same way.  The confidence level is the percentage of times you expect to reproduce an estimate between the upper and lower bounds of the confidence interval, and is set by the alpha value. Confidence, in statistics, is another way to describe probability. For example, if you construct a confidence interval with a 95% confidence level, you are confident that 95 out of 100 times the estimate will fall between the upper and lower values specified by the confidence interval. Your desired confidence level is usually one minus the alpha ( a ) value you used in your statistical test: Confidence level = 1 − a So if you use an alpha value of p &lt; 0.05 for statistical significance, then your confidence level would be 1 − 0.05 = 0.95, or 95%. 10.3.2 Proof for Confidence Intervals 10.3.2.1 CI for Population Proportion Let’s say we want to estimate the population proportion (the mean of a categorical variable) e.g., the proportion of votes a Democratic candidate gets in an election. In our example, the true population proportion p = 0.45. In real life we actually don’t know this parameter. In R we can create a Monte Carlo Simulation (creating confidence intervals 10,000 times) in order to prove the validity of confidence intervals in general. p = 0.45 n = 1000 B = 10000 set.seed(1) correct_list = replicate(B, { x = sample(c(1,0), size = n, replace = T, prob = c(p, (1-p))) x_hat = mean(x) se_hat = sqrt(x_hat * (1 - x_hat) / n) between(p, x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat) }) And how often did we get a confidence interval that did in fact include the real population parameter p? The confidence interval did include the true parameter 0.9482 of the time. Pretty close to the theoretical goal of 0.95! mean(correct_list) ## [1] 0.9482 Let’s visualize the validity of CIs with another 100 samples and ggplot2. # Used to create the graph lower_bounds = c() upper_bounds = c() x_hats = c() true = c() # Small Monte Carlo simulation for (i in 1:100) { x = sample(c(1,0), size = n, replace = T, prob = c(p, (1-p))) x_hat = mean(x) x_hats = c(x_hats, x_hat) se_hat = sqrt(x_hat * (1 - x_hat) / n) lower_bounds = c(lower_bounds, (x_hat - 1.96 * se_hat)) upper_bounds = c(upper_bounds, (x_hat + 1.96 * se_hat)) true = c(true, between(p, x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)) } # ggplot2 only accepts tables table = tibble(x_hats, lower_bounds, upper_bounds, true) # Graph highlighting the validity of a 95% CI ggplot(table, aes(seq_along(x_hats), x_hats)) + geom_pointrange(aes(ymin = lower_bounds, ymax = upper_bounds, color = true)) + geom_hline(yintercept = 0.45) The proof from our 10,000 confidence intervals above that did in fact capture the true population proportion \\(p\\) (0.45) in approximately 95% of the time (0.943) tells us that we can create confidence intervals from a single sample “with confidence.” In other words, we can in fact be sure that a 95% confidence interval constructed from our sample \\(\\hat{x}\\) does include \\(p\\) in 95% of the time. Here is a single sample from which we will create a 95% confidence interval to showcase a more real-life example. # In real life this model is shrouded in unknowns x = sample(c(1,0), size = 1000, replace = T, prob = c(p, (1-p))) # Sample mean x_hat = mean(x) # Standard Error se_hat = sqrt(x_hat * (1 - x_hat) / n) # Creating the bounds of the CI lower_bounds = x_hat - 1.96 * se_hat upper_bounds =x_hat + 1.96 * se_hat # The confidence interval is: lower_bounds ## [1] 0.4181713 upper_bounds ## [1] 0.4798287 The confidence interval did in fact capture the true population parameter, which is actually unknown. So the actual result would be expressed as follows: We are 95% confident that the true population proportion is between 0.404 and 0.465. 10.3.2.2 CI for Population Mean Lets say we a population of monthly incomes that is totally random and non-normally distributed. set.seed(1) # Create 10000 random &quot;incomes&quot; incomes = runif(10000, min = 800, max = 6000) # Peek at it head(incomes, 10) ## [1] 2180.645 2735.044 3778.837 5522.681 1848.746 5471.626 5712.311 4236.149 ## [9] 4071.393 1121.289 # Create true population mean (unknown in real life) mu = mean(incomes) mu ## [1] 3400.873 # Plot reveals its very much unnormal plot(density(incomes)) Now comes the confidence interval. The parent population is very much non-normal, but because of the CTL and random sampling we can assume an approximately normal distribution of the sampling distribution of the sample means. Further, we do not replace picks when sampling (just like in real life), but can still assume independence because the sample size of 100 is only 1% from the total population of 10,000. # Sample x = sample(incomes, 100) # Sample mean x_hat = mean(x) # Sample standard error se_hat = sd(x) / sqrt(100) # Create the margin of error lower_bound = x_hat - 1.96 * se_hat upper_bound = x_hat + 1.96 * se_hat ci = c(lower_bound, upper_bound) # Here is our confidence interval ci ## [1] 3260.147 3864.235 # Here is the real population mean (unknown in real life) mu ## [1] 3400.873 Result: The 95% confidence interval for the true population mean \\(\\mu\\) is 3212.27 to 3816.358 (sample mean is 3514.314). t.test(x, conf.level = 0.95)$conf.int ## [1] 3256.415 3867.967 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 10.4 Hypothesis Testing Hypothesis testing is a form of statistical inference that uses data from a sample to draw conclusions about a population parameter or a population probability distribution. First, a tentative assumption is made about the parameter or distribution. This assumption is called the null hypothesis and is denoted by \\(H_0\\). An alternative hypothesis (denoted \\(H_a\\)), which is the opposite of what is stated in the null hypothesis, is then defined. The hypothesis-testing procedure involves using sample data to determine whether or not \\(H_0\\) can be rejected. If \\(H_0\\) is rejected, the statistical conclusion is that the alternative hypothesis \\(H_a\\) is true. 10.4.1 P-Value We can think of a P-value as a conditional probability: given the null hypothesis is true, what’s the probability of obtaining a sample statistic as extreme or more than the one observed by random chance alone. In this type of test, we use the alternative hypothesis \\(H_{\\mathrm{a}}\\) to decide if the P-value comes from the probability above the test statistic, below the test statistic, or comes from a two-sided probability. 10.4.2 Z-Test 10.4.3 Formula: z test statistic We can calculate the test statistic corresponding to the sample result: \\[ z=\\frac{\\text { statistic - parameter }}{\\text { standard deviation of statistic }} \\] \\[ =\\frac{\\hat{p}-p_{0}}{\\sqrt{\\frac{p_{0}\\left(1-p_{0}\\right)}{n}}} \\] (where \\(\\hat{p}\\) is the sample proportion, \\(p_{0}\\) is the proportion from the null hypothesis, and \\(n\\) is the sample size). 10.4.4 R: P-value from z-score R Function To find the p-value associated with a z-score in R, we can use the pnorm() function, which uses the following syntax: pnorm(q, mean = 0, sd = 1, lower.tail = TRUE) where: q: The z-score mean: The mean of the normal distribution. Default is 0. sd: The standard deviation of the normal distribution. Default is 1. lower.tail: If TRUE, the probability to the left of q in the normal distribution is returned. If FALSE, the probability to the right is returned. Default is TRUE. Example 1: one-sided In 2011, \\(51 \\%\\) of cell phone owners in a country reported that their cell phone was a smartphone. The following year, the researchers wanted to test \\(H_{0}: p=0.51\\) versus \\(H_{\\mathrm{a}}: p&gt;0.51\\), where \\(p\\) is the proportion of cell phone owners in that country who have a smartphone. They surveyed a random sample of 934 cell phone owners in that country and found that 501 of them had a smartphone. The test statistic for these results was \\(z \\approx 1.61\\). What is the P-Value? pnorm(1.61, lower.tail = F) ## [1] 0.05369893 P-Value Answer: 0.054 Example 2: two-sided Amanda read a report saying that \\(49 \\%\\) of teachers in the United States were members of a labor union. She wanted to test whether this was true in her state, so she took a random sample of 300 teachers from her state to test \\(H_{0}: p=0.49\\) versus \\(H_{\\mathrm{a}}: p \\neq 0.49\\), where \\(p\\) is the proportion of teachers in her state who are members of a labor union. The sample results showed 134 teachers were members of a labor union, and the corresponding test statistic was \\(z \\approx-1.50\\). What is the P-Value? 2*pnorm(-1.5) ## [1] 0.1336144 P-Value Answer: 0.133 10.4.5 Conditions for mean inference When we want to carry out inference (build a confidence interval or do a significance test) on a mean, the accuracy of our methods depends on a few conditions. Before doing the actual computations of the interval or test, it’s important to check whether or not these conditions have been met. Otherwise the calculations and conclusions that follow may not be correct. The conditions we need for inference on a mean are: Random: A random sample or randomized experiment should be used to obtain the data. Normal: The sampling distribution of \\(\\overline{x}\\) (the sample mean) needs to be approximately normal. This is true if our parent population is normal or if our sample is reasonably large \\((n\\geq 30)\\). Independent: Individual observations need to be independent. If sampling without replacement, our sample size shouldn’t be more than \\(10\\%\\) of the population. Let’s look at each of these conditions a little more in-depth. The random condition Random samples give us unbiased data from a population. When we don’t use random selection, the resulting data usually has some form of bias, so using it to infer something about the population can be risky. For example, suppose a university wants to report the average starting salary of their graduates. How do they obtain the data? They can’t access the salaries of all graduates, and they can’t realistically get salaries from a random sample of graduates. The university could rely on graduates who are willing to share their salaries to calculate the average, but using voluntary response will likely lead to a biased estimate of the true average. Graduates with higher starting salaries will probably be more willing to report their salaries than graduates with low salaries (or graduates without salaries). Also, graduates who participate may claim their salary is higher than it really is, but they’d be unlikely to say it’s lower than it really is. The big idea is that data that came from a non-random sample may not be representative of its population. More specifically, sample means are unbiased estimators of their population mean. For example, suppose we have a bag of ping pong balls individually numbered from 0 to 30, so the population mean of the bag is 15. We could take random samples of balls from the bag and calculate the mean from each sample. Some samples would have a mean higher than 15 and some would be lower. But on average, the mean of each sample will equal 15. We write this property as \\(\\mu_{\\bar{x} } =\\mu\\), which holds true as long as we are taking random samples. This won’t necessarily happen if we use a non-random sample. Biased samples can lead to inaccurate results, so they shouldn’t be used to create confidence intervals or carry out significance tests. The normal condition The sampling distribution of \\(\\bar{x}\\) (a sample mean) is approximately normal in a few different cases. The shape of the sampling distribution of \\(\\bar{x}\\) mostly depends on the shape of the parent population and the sample size n. Case 1: Parent population is normally distributed If the parent population is normally distributed, then the sampling distribution of \\(\\bar{x}\\) is approximately normal regardless of sample size. So if we know that the parent population is normally distributed, we pass this condition even if the sample size is small. In practice, however, we usually don’t know if the parent population is normally distributed. Case 2: Not normal or unknown parent population; sample size is large \\((n \\geq 30)\\) The sampling distribution of \\(\\bar{x}\\) is approximately normal as long as the sample size is reasonably large. Because of the central limit theorem, when \\(n \\geq 30\\), we can treat the sampling distribution of \\(\\bar{x}\\) as approximately normal regardless of the shape of the parent population. There are a few rare cases where the parent population has such an unusual shape that the sampling distribution of the sample mean \\(\\bar{x}\\) isn’t quite normal for sample sizes near 30. These cases are rare, so in practice, we are usually safe to assume approximately normality in the sampling distribution when \\(n \\geq 30\\). Case 3: Not normal or unknown parent population; sample size is small \\(n&lt;30\\) As long as the parent population doesn’t have outliers or strong skew, even smaller samples will produce a sampling distribution of \\(\\bar{x}\\) that is approximately normal. In practice, we can’t usually see the shape of the parent population, but we can try to infer shape based on the distribution of data in the sample. If the data in the sample shows skew or outliers, we should doubt that the parent is approximately normal, and so the sampling distribution of \\(\\bar{x}\\) may not be normal either. But if the sample data are roughly symmetric and don’t show outliers or strong skew, we can assume that the sampling distribution of \\(\\bar{x}\\) will be approximately normal. The big idea is that we need to graph our sample data when \\(n &lt; 30\\), is less than, 30 and then make a decision about the normal condition based on the appearance of the sample data. The independence condition To use the formula for standard deviation of \\(\\bar{x}\\) we need individual observations to be independent. In an experiment, good design usually takes care of independence between subjects (control, different treatments, randomization). In an observational study that involves sampling without replacement, individual observations aren’t technically independent since removing each observation changes the population. However the 10% condition says that if we sample 10% or less of the population, we can treat individual observations as independent since removing each observation doesn’t change the population all that much as we sample. For instance, if our sample size is \\(n=30\\) there should to be at least \\(N = 300\\) members in the population for the sample to meet the independence condition. Assuming independence between observations allows us to use this formula for standard deviation of \\(\\bar{x}\\) when we’re making confidence intervals or doing significance tests: \\[ \\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}} \\] We usually don’t know the population standard deviation \\(\\sigma\\), so we substitute the sample standard deviation \\(s_x\\) as an estimate for \\(\\sigma\\). When we do this, we call it the standard error of \\(\\bar{x}\\) to distinguish it from the standard deviation. So our formula for standard error of \\(\\bar{x}\\) is: \\[ \\sigma_{\\bar{x}} \\approx \\frac{s_{x}}{\\sqrt{n}} \\] Summary If all three of these conditions are met, then we can we feel good about using \\(t\\) distributions to make a confidence interval or do a significance test. Satisfying these conditions makes our calculations accurate and conclusions reliable. The random condition is perhaps the most important. If we break the random condition, there is probably bias in the data. The only reliable way to correct for a biased sample is to recollect the data in an unbiased way. The other two conditions are important, but if we don’t meet the normal or independence conditions, we may not need to start over. For example, there is a way to correct for the lack of independence when we sample more than 10% of a population, but it’s beyond the scope of what we’re learning right now. The main idea is that it’s important to verify certain conditions are met before we make these confidence intervals or do these significance tests. 10.4.6 Formula: t test statistic The test statistic gives us an idea of how far away our sample result is from our null hypothesis. For a one-sample t test for a mean, our test statistics is: \\[ \\begin{aligned}t &amp;=\\frac{\\text { statistic }-\\text { parameter }}{\\text { standard error of statistic }} \\\\&amp;=\\frac{\\bar{x}-\\mu_{0}}{\\frac{s_{x}}{\\sqrt{n}}}\\end{aligned} \\] The statistic \\(\\bar{x}\\) is the sample mean, and the parameter \\(\\mu_{0}\\) is the mean from the null hypothesis. The standard error of the sample mean is \\(s_{x}\\) (the sample standard deviation) divided by the square root of \\(n\\) (the sample size). 10.4.7 R: P-value from t statistic Degrees of freedom \\(n - 1\\) The function in R: pt() pt(q = 2, # the t-statistic df = 5, # the degrees of freedom lower.tail = T # probabilities for lower or upper tail? Default is True. ) ## [1] 0.9490303 Example 1: one-sided Daisy was testing \\(H_{0}: \\mu=33\\) versus \\(H_{\\mathrm{a}}: \\mu&gt;33\\) with a sample of 11 observations. Her test statistic was \\(t=1.368\\). Assume that the conditions for inference were met. What is the P-value? pt(q = 1.368, df = 10, lower.tail = F) ## [1] 0.100632 Example 2: two-sided Jasper was testing \\(H_{0}: \\mu=36\\) versus \\(H_{\\mathrm{a}}: \\mu \\neq 36\\) with a sample of 16 observations. His test statistic was \\(t=2.4\\). Assume that the conditions for inference were met. What is the P-value? 2*pt(q = 2.4, df = 15, lower.tail = F) ## [1] 0.02982493 10.4.8 R: t-tests x = rnorm(10) y = rnorm(10) t.test(x,y) ## ## Welch Two Sample t-test ## ## data: x and y ## t = -1.0676, df = 17.996, p-value = 0.2998 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.2896717 0.4205834 ## sample estimates: ## mean of x mean of y ## -0.1968400 0.2377042 10.4.9 t or z Statistic? z-tests are a statistical way of testing a hypothesis when either: We know the population variance, or We do not know the population variance but our sample size is large n ≥ 30 If we have a sample size of less than 30 and do not know the population variance, then we must use a t-test. The t-test is kind of a weaker statistic, but which helps us if we have less information available. Proportion significance tests don’t need t-tests, because we can calculate the z-score without the sampling distribution or population standard deviation. 10.5 Correlation 10.6 Regression "],["causal-inference.html", "11 Causal Inference 11.1 Potential outcome framework", " 11 Causal Inference The Rubin causal model is based on the idea of potential outcomes. For example, a person would have a particular income at age 40 if they had attended college, whereas they would have a different income at age 40 if they had not attended college. To measure the causal effect of going to college for this person, we need to compare the outcome for the same individual in both alternative futures. Since it is impossible to see both potential outcomes at once, one of the potential outcomes is always missing. This dilemma is the “fundamental problem of causal inference.” 11.1 Potential outcome framework Thought experiment Let’s assume we want to know if a particular intervention/treatment $D$ (like job loss) caused a particular outcome $Y$ (AFD vote propensity) The variables job loss $D$ can take two values: 1 if the respondent did lose the job and 0 otherwise The variable AFD vote propensity $Y$ is measured as a standard normal variable (that could be a continous survey measure) In this thought experiment, we also assume that we know the potential outcomes for each individual Our experimental sample of interest are $N=100$ German citizens $u$ drawn from the population of all citizers that are eligble to vote In an ideal experiment we would observe the vote choice under job loss or not and compute the difference to get the causal effect Lets simulate this setup in R: N=100 u = seq(1:N) Y0 = rnorm(N) Y1 = rnorm(N) + 1 D=1:100 %in% sample(1:100, 50) yl = &quot;Y(0) &amp; Y(1)&quot; # Plot Potential Outcomes for each unit # Library to resize plot library(repr) plot(u, Y0, ylim=c(-3, 4), xlim=c(1,N), xlab=&quot;u&quot;) lines(u, Y1, type = &quot;p&quot;, col=&quot;red&quot;) title(&quot;Y(1) and Y(0) for all units &quot;) options(repr.plot.width=4, repr.plot.height=4) # Plot Potential Outcomes for each unit and take mean for treatment and control plot(u[D==0], Y0[D==0], ylim=c(-3, 4), xlim=c(1,N), main = &quot;Y(1| T=1) and Y(0| T=0)&quot;, xlab=&quot;u&quot;, ylab=yl) abline(h=mean(Y0[D==0])) lines(u[D==1], Y1[D==1], type = &quot;p&quot;, col=&quot;red&quot;) abline(h=mean(Y1[D==1]), col=&quot;red&quot;) options(repr.plot.width=4, repr.plot.height=4) "],["functions.html", "12 Functions 12.1 Read and Write 12.2 Random 12.3 Order 12.4 Visualization 12.5 Conditionals 12.6 Numbers 12.7 Apply Functions 12.8 String Functions", " 12 Functions 12.1 Read and Write read.table() Reads csv files. read.table(file, header = FALSE, # Is the first line filled with names? sep = &quot;&quot;, # Can be &quot;,&quot; or &quot;;&quot; or &quot;\\tab&quot; or &quot; &quot;. quote = &quot;\\&quot;&#39;&quot;, dec = &quot;.&quot;, numerals = c(&quot;allow.loss&quot;, &quot;warn.loss&quot;, &quot;no.loss&quot;), row.names, col.names, as.is = !stringsAsFactors, na.strings = &quot;NA&quot;, colClasses = NA, # Here you can add nrows = -1, # -1 means use all rows from the first, 0, to the last, -1. skip = 0, # Skip n number of lines. check.names = TRUE, fill = !blank.lines.skip, strip.white = FALSE, blank.lines.skip = TRUE, comment.char = &quot;#&quot;, allowEscapes = FALSE, flush = FALSE, stringsAsFactors = FALSE, fileEncoding = &quot;&quot;, encoding = &quot;unknown&quot;, text, skipNul = FALSE) 12.2 Random setwd() Setting the working directory via relative path. setwd(&quot;~/Documents/R/Working Directory&quot;) And via absolute path. setwd(&quot;/Users/dinocuric/Documents/R/Working Directory&quot;) ls() Lists all object in environment. ls() # [1] &quot;table1&quot; &quot;my_list&quot; &quot;table2&quot; :: What if we want to use the stats filter instead of the dplyr filter but dplyr appears first in the search list? You can force the use of a specific namespace by using double colons (::) like this: stats::filter install.packages() Install new packages install.packages(&quot;package_name&quot;)` Note: package name needs to be in quotes. library() Enable installed packages: library(package_name) Or mostly interchangeable: require(package_name) Note: No “” are needed when enabling a package in your library. rm() Remove objects from the environment. rm(object) sample() Create random integer numbers by selecting a sample from a population. First argument is the pool, the second is the sample size. sample(1:100,5) # [1] 87 31 69 1 85 c() Combine values into a vector. c(1,15,6,1,6,72,3,7,3) # [1] 1 15 6 1 6 72 3 7 3 rep() Replicate values a specific number of times. rep(10, 5) # [1] 10 10 10 10 10 rep(1:5, 3) # [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 max(), which.max() with() For a quick plot that avoids accessing variables twice, we can use the with function. The function with lets us use the murders column names in the plot function. It also works with any data frames and any function. with(murders, plot(population, total)) max() Return the highest value in a numeric vector. max(murders$total) # [1] 1257 Use min()for opposite. which() Suppose we want to look up California’s row. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type: ind &lt;- which(murders$state == &quot;California&quot;) ind # 5 murders[ind,] # state abb region population total rank # 5 California CA West 37253956 1257 51 match() If instead of just one state we want to get the rows for several states, say New York, Florida, and Texas, we can use the function match(). This function tells us which indexes of a second vector match each of the entries of a first vector: ind &lt;- match(c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;), murders$state) ind # [1] 33 10 44 murders[ind,] # state abb region population total rank # 33 New York NY Northeast 19378102 517 48 # 10 Florida FL South 19687653 669 49 # 44 Texas TX South 25145561 805 50 %in% If rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let’s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this: c(&quot;Boston&quot;, &quot;Dakota&quot;, &quot;Washington&quot;) %in% murders$state #&gt; [1] FALSE FALSE TRUE Advanced: There is a connection between match and %in% through which. To see this, notice that the following two lines produce the same index (although in different order): match(c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;), murders$state) #&gt; [1] 33 10 44 which(murders$state%in%c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;)) #&gt; [1] 10 33 44 attach() To use variables in an object without needing the $ sign. Do not forget the detach function to end it. # Begin attach(mtcars) cyl # Otherwise mtcars$cyl hp names(mtcars) # At the end detach(mtcars) head() Show the first or last 6 rows in a data set. head(women) # First 6 rows. tail(women, 2) # Last two rows. colnames() and rownames() Access column and row names. colnames(starwars) # &quot;name&quot; &quot;height&quot; &quot;mass&quot; &quot;hair_color&quot; &quot;skin_color&quot; &quot;eye_color&quot; &quot;birth_year&quot; rownames(starwars) # &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; factor() Creates a factor. Custom levels and, or labels are optional. gender = c(&quot;female&quot;,&quot;male&quot;,&quot;male&quot;,&quot;non-binary&quot;,&quot;non-binary&quot;) gender = factor(gender, levels = c(&quot;female&quot;,&quot;male&quot;,&quot;non-binary&quot;)) gender # [1] female male male non-binary non-binary # Levels: female male non-binary You could also use character labels together with numeric levels to use certain mathematical operations. gender = c(0,1,1,2,2) gender = factor(gender, levels = c(0,1,2), labels = c(&quot;Female&quot;, &quot;Male&quot;, &quot;Non-Binary&quot;)) gender # [1] Female Male Male Non-Binary Non-Binary # Levels: Female Male Non-Binary levels() This function is used to explicitly access the level part of a factor. levels(gender) # [1] &quot;Female&quot; &quot;Male&quot; &quot;Non-Binary&quot; And it let’s us change its values. levels(gender) = c(&quot;female&quot;,&quot;male&quot;,&quot;non-binary&quot;) gender # [1] female male male non-binary non-binary # Levels: female male non-binary 12.3 Order sort() Sorts a vector by increasing or decreasing. x = c(31,4,15,92,65) sort(x) # [1] 4 15 31 65 92 Use decreasing parameter to change order. order() Creates the sorting vector which can be used to order other variables or data frames as a whole. order(murders$total) # [1] 46 35 30 51 12 42 20 13 27 40 2 16 45 49 28 38 8 24 17 6 32 29 4 48 # [25] 7 50 9 37 18 22 25 1 15 41 43 3 31 47 34 21 36 26 19 14 11 23 39 33 # [49] 10 44 5 murders[order(murders$total),] # state abb region population total # 46 Vermont VT Northeast 625741 2 # 35 North Dakota ND North Central 672591 4 # 30 New Hampshire NH Northeast 1316470 5 # 51 Wyoming WY West 563626 5 # ... Use decreasing parameter to change filtering order. rank() Although not as frequently used as order and sort, the function rank is also related to order and can be useful. For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example: x &lt;- c(31, 4, 15, 92, 65) rank(x) #&gt; [1] 3 1 2 5 4 We could use `rank()` to create a new variable storing the rank of each observation in relation to some value. murders$rank = rank(murders$total) murders$rank # [1] 32.0 11.0 36.0 23.5 51.0 20.0 25.5 17.0 27.0 49.0 45.0 5.0 8.5 44.0 # [15] 33.0 12.0 19.0 29.0 43.0 7.0 40.0 30.0 46.0 18.0 31.0 42.0 8.5 15.0 # [29] 22.0 3.5 37.0 21.0 48.0 39.0 2.0 41.0 28.0 16.0 47.0 10.0 34.0 6.0 # [43] 35.0 50.0 13.0 1.0 38.0 23.5 14.0 25.5 3.5 To summarize, let’s look at the results of the three functions we have introduced: original sort order rank 31 4 2 3 4 15 3 1 15 31 1 2 92 65 5 5 65 92 4 4 reorder() This function reorders the levels of a factor. This is different from order() and sort(), where it sorts each individual observation. Factors and their order are central for visualization. The “default” method treats its first argument as a categorical variable, and reorders its levels based on the values of a second variable, usually numeric. data %&gt;% ggplot(aes(year,population, color = reorder(country, desc(population)))) + # &quot;Reorder countries by descending order of population&quot; geom_line() ` Another way to use `reorder()` is to (re)mutate the factor variable before`ggplot`. ```r murders %&gt;% mutate(state = reorder(state, murder_rate)) %&gt;% # Reorder state factor levels by ascending order of murder_rate ggplot(aes(state, murder_rate)) + geom_bar(stat=&quot;identity&quot;) which.max() Determines the location, i.e., index of the minimum or maximum value of a numeric vector. Is basically a filter for one value. which.max(murders$rate) # [1] 5 In a data frame used to index the specific row. murders[which.max(murders$total),] # state abb region population total # 5 California CA West 37253956 1257 Use which.min()for the opposite. 12.4 Visualization plot() plot( data = data, # Dataset x = gdp, # X variable y = population, # Y variable main = &quot;Maximum Temperatures in a Week&quot;, # Title xlab = &quot;Degree Celsius&quot;, # X axis label ylab = &quot;Day&quot;, # Y axis label type = &quot;l&quot; # Use &quot;l&quot; to plot a linear regression line istead of points ) Normal Distribution and linear graph Create x and y variables which represent a normal distribution. # Create a sequence of numbers between -10 and 10 incrementing by 0.1. x &lt;- seq(-10, 10, by = .1) # Choose the mean as 2.5 and standard deviation as 0.5. y &lt;- dnorm(x, mean = 2.5, sd = 0.5) Scatterplot it. plot(x,y) boxplot() boxplot( data$variable, # Numeric countinous variable main = &quot;Maximum Temperatures in a Week&quot;, # Title xlab = &quot;Degree Celsius&quot;, # X axis label ylab = &quot;Day&quot;, # Y axis label horizontal = T # Y instead of x axis, i.e. horizontal outline = # Hide outliers ) abline() GDP and internet usage. plot(x = UN$GDP, y = UN$Internet) # Pretty strong positive correlation. abline(lm(Internet ~ GDP, UN)) GDP and fertility rates. plot(UN$GDP, UN$Fertil) # Not so strong negative correlation. abline(lm(data = UN, Fertil ~ GDP)) GDP and GII (female inequality index). plot(UN$GDP, UN$GII) # Pretty strong negative correlation. abline(lm(data = UN, GII ~ GDP)) barplot() barplot( data$variable, # Variable with max 6 distinct values/categories main = &quot;Maximum Temperatures in a Week&quot;, # Title xlab = &quot;Degree Celsius&quot;, # X axis label ylab = &quot;Day&quot;, # Y axis label names.arg = c(&quot;Sun&quot;, &quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;), # Levels labels col = &quot;darkred&quot;, # Column colors (fill) border=&quot;red&quot;, # Column border colors (color) horiz = TRUE, # Y axis instead of x axis, as by default ) hist() hist( data$variable, # ? main = &quot;Maximum Temperatures in a Week&quot;, # Title xlab = &quot;Degree Celsius&quot;, # X axis label ylab = &quot;Day&quot;, # Y axis label breaks = 10 # Change the number of intervals # breaks = c(55,60,70,75,80,100) # Or this way to map exactly where each break should be xlim = c(50,100), # Limit x axis scale col = &quot;darkmagenta&quot;, # Change column color border = &quot;red&quot;, # Column border colors (color) freq = FALSE # ) pie() pie(table(cat$race)) stem() This figure, called a stem-and-leaf plot, represents each observation by its leading digit(s) (the stem) and by its final digit (the leaf). Each stem is a number to the left of the vertical bar and a leaf is a number to the right of it. For instance, on the first line, the stem of 1 and the leaves of 2 and 3 represent the violent crime rates 12 and 13. The plot arranges the leaves in order on each line, from smallest to largest. Stem-and-leaf plots are useful for quick portrayals of small data sets. stem(cat$income) 12.5 Conditionals ifelse() This function takes three arguments: a logical and two possible answers. If the logical is TRUE, the value in the second argument is returned and if FALSE, the value in the third argument is returned. Here is an example: a &lt;- 0 ifelse(a &gt; 0, 1/a, NA) #&gt; [1] NA The function is particularly useful because it works on vectors. It examines each entry of the logical vector and returns elements from the vector provided in the second argument, if the entry is TRUE, or elements from the vector provided in the third argument, if the entry is FALSE. a &lt;- c(0, 1, 2, -4, 5) ifelse(a &gt; 0, 1/a, NA) This table helps us see what happened: # a is_a_positive answer1 answer2 result # 0 FALSE Inf NA NA # 1 TRUE 1.00 NA 1.0 # 2 TRUE 0.50 NA 0.5 #-4 FALSE -0.25 NA NA # 5 TRUE 0.20 NA 0.2 Here is an example of how this function can be readily used to replace all the missing values in a vector with zeros: data(na_example) no_nas &lt;- ifelse(is.na(na_example), 0, na_example) sum(is.na(no_nas)) #&gt; [1] 0 any() The any function takes a vector of logicals and returns TRUE if any of the entries is TRUE. z &lt;- c(TRUE, TRUE, FALSE) any(z) #&gt; [1] TRUE all() The all function takes a vector of logicals and returns TRUE if all of the entries are TRUE. z &lt;- c(TRUE, TRUE, FALSE) all(z) #&gt; [1] FALSE is.na() Takes a vector and returns a vector of logicals with TRUE if an element is NA and FALSE if it’s not NA. is.na(na_example) # [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE # [12] TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE # [23] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE # ... identical() identical(1,2) # [1] FALSE identical(1,1) # [1] TRUE 12.6 Numbers summary() Produces summary statistics for a variable. summary(women$height) # Min. 1st Qu. Median Mean 3rd Qu. Max. # 58.0 61.5 65.0 65.0 68.5 72.0 Or for a whole dataframe. summary(dat) # iso2c country EN.ATM.CO2E.KT year # Length:212 Length:212 Min. : 63116 Min. :1960 # Class :character Class :character 1st Qu.: 329931 1st Qu.:1973 # Mode :character Mode :character Median : 483860 Median :1986 # Mean :1676369 Mean :1986 # 3rd Qu.:3662251 3rd Qu.:1999 # Max. :5776410 Max. :2012 # NA&#39;s :30 round() Round values in a vector to specific digits. round(1.12345, digits = 2) # 1.12 sum() Sum of the elements in an object. sum(vector) rnorm() Create random numbers around a specific mean and standard deviation. rnorm(n = 10, mean = 5, sd = 3) # [1] 8.142960 5.711236 7.952512 4.343039 5.587903 10.592649 1.129671 8.597915 7.836368 2.016112 runif() Create random numbers from a minimum to a maximum value. runif(n = 10, min = 1, max = 10) # [1] 7.848954 8.616900 2.975886 9.056828 1.892161 5.684384 3.630172 5.797314 1.330543 7.122972 seq() Create sequence of numbers by specific steps. seq(from = 1, to = 10, by = 3) # [1] 1 4 7 10 12.7 Apply Functions Function from the apply family (apply(), sapply(), lapply()) are a special functionality in R. They in large replace complex for loops. apply() Example data Chicago.weather Return a vector by applying a function (like mean) on all rows or columns of a matrix. 1 means applying it on all rows, 2 on all columns. apply(Chicago.weather, 1, mean) apply(Chicago.weather, 2, mean) # Doesn&#39;t make sense.   and lapply() Here is a list with weather data for four cities/components. Weather.list We can use the brackets in sapply() and lapply() to return specific rows and columns in all components in a list. sapply(Weather.list, &quot;[&quot;, 1,) # Returns the first row and all columns Weather.list$Chicago[1,] # Check. lapply(Weather.list, &quot;[&quot;, 1:3,4) # Return rows 1 to 3 and their fourth column in each component. Anonymous/Nested Functions Example data for a vector: v &lt;- 1:5 Also possible with a vector in which every element, like in a for loop, needs to changed. sapply() returns a vector or table when using a vector or a list. lapply() returns a list when using a vector, table, or list. v sapply(v, function(element_in_v) element_in_v +1) lapply(v, function(element_in_v) element_in_v +1) 12.8 String Functions nchar() Returns the number of characters in a vector. nchar(&quot;Dino Curic&quot;) # 10 grepl() grepl() returns a logical indicating if the pattern was found 2. grep() returns a vector of index locations of matching pattern instances. text &lt;- &quot;Hi there, do you know who you are voting for?&quot; grepl(&#39;voting&#39;,text) grepl(&#39;Sammy&#39;,text) v &lt;- c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;) grep(&quot;d&quot;, v) paste() Paste converts its arguments (via as.character) to character strings, and concatenates them (separating them by the string given by sep). Basically the combine function with character type features. greeting &lt;- &quot;Hello&quot; name &lt;- &quot;Frank&quot; paste(greeting, name, sep = &quot;, &quot;) sub() + gsub() To remove and replace characters we can use the functions sub(), to remove the first instance of it, or gsub(), to remove every instance. The first arguement aks for the character which should be removed. The second aks with what? And the third for where to look for. If the second argument is \"\" then the function it’s basically a remove function. # Remove dollar sign. Note: [ ] brackets needed for special characters like $. rev_exp$Revenue &lt;- gsub(&quot;[$]&quot;, &quot;&quot;, rev_exp$Revenue) Remove commas rev_exp$Revenue &lt;- gsub(&quot;,&quot;, &quot;&quot;, rev_exp$Revenue) Remove ” Dollars” rev_exp$Expenses &lt;- gsub(&quot; Dollars&quot;, &quot;&quot;, rev_exp$Expenses) And lastly, convert the two variables to numeric. rev_exp$Revenue &lt;- as.numeric(rev_exp$Revenue) rev_exp$Expenses &lt;- as.numeric(rev_exp$Expenses) rev_exp "],["frequent-problems.html", "13 Frequent Problems 13.1 First exploration of new dataset 13.2 A Count and prop table 13.3 Bar graph with count data 13.4 Bar graph with percentage labels 13.5 Collapse factors to „Other” 13.6 Filter for specific values 13.7 Change bar colors in barplot 13.8 Hide aes(color) mapping legend 13.9 Re-code values of categorical variables 13.10 Order color legend 13.11 Show unique values 13.12 Slice rows by maximum or minimum values 13.13 Show Number of NAs 13.14 Drop rows with missing values 13.15 Replace NAs 13.16 The factor variable trap", " 13 Frequent Problems 13.1 First exploration of new dataset The skim()shows how many missing and unique values each variable has. It uses appropriate measures to describe each variable based on its type: character, numeric or list. skimr::skim(starwars) Table 13.1: Data summary Name starwars Number of rows 87 Number of columns 14 _______________________ Column type frequency: character 8 list 3 numeric 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace name 0 1.00 3 21 0 87 0 hair_color 5 0.94 4 13 0 12 0 skin_color 0 1.00 3 19 0 31 0 eye_color 0 1.00 3 13 0 15 0 sex 4 0.95 4 14 0 4 0 gender 4 0.95 8 9 0 2 0 homeworld 10 0.89 4 14 0 48 0 species 4 0.95 3 14 0 37 0 Variable type: list skim_variable n_missing complete_rate n_unique min_length max_length films 0 1 24 1 7 vehicles 0 1 11 0 2 starships 0 1 17 0 5 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist height 6 0.93 174.36 34.77 66 167.0 180 191.0 264 ▁▁▇▅▁ mass 28 0.68 97.31 169.46 15 55.6 79 84.5 1358 ▇▁▁▁▁ birth_year 44 0.49 87.57 154.69 8 35.0 52 72.0 896 ▇▁▁▁▁ The glimpse function, on the other hand, gives us a good peak at the first raw values each variable has. glimpse(starwars) ## Rows: 87 ## Columns: 14 ## $ name &lt;chr&gt; &quot;Luke Skywalker&quot;, &quot;C-3PO&quot;, &quot;R2-D2&quot;, &quot;Darth Vader&quot;, &quot;Leia Or… ## $ height &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2… ## $ mass &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.… ## $ hair_color &lt;chr&gt; &quot;blond&quot;, NA, NA, &quot;none&quot;, &quot;brown&quot;, &quot;brown, grey&quot;, &quot;brown&quot;, N… ## $ skin_color &lt;chr&gt; &quot;fair&quot;, &quot;gold&quot;, &quot;white, blue&quot;, &quot;white&quot;, &quot;light&quot;, &quot;light&quot;, &quot;… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;, &quot;yellow&quot;, &quot;brown&quot;, &quot;blue&quot;, &quot;blue&quot;,… ## $ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, … ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;none&quot;, &quot;none&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;,… ## $ gender &lt;chr&gt; &quot;masculine&quot;, &quot;masculine&quot;, &quot;masculine&quot;, &quot;masculine&quot;, &quot;femini… ## $ homeworld &lt;chr&gt; &quot;Tatooine&quot;, &quot;Tatooine&quot;, &quot;Naboo&quot;, &quot;Tatooine&quot;, &quot;Alderaan&quot;, &quot;T… ## $ species &lt;chr&gt; &quot;Human&quot;, &quot;Droid&quot;, &quot;Droid&quot;, &quot;Human&quot;, &quot;Human&quot;, &quot;Human&quot;, &quot;Huma… ## $ films &lt;list&gt; &lt;&quot;The Empire Strikes Back&quot;, &quot;Revenge of the Sith&quot;, &quot;Return… ## $ vehicles &lt;list&gt; &lt;&quot;Snowspeeder&quot;, &quot;Imperial Speeder Bike&quot;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &quot;Imp… ## $ starships &lt;list&gt; &lt;&quot;X-wing&quot;, &quot;Imperial shuttle&quot;&gt;, &lt;&gt;, &lt;&gt;, &quot;TIE Advanced x1&quot;,… 13.2 A Count and prop table First way with forcats::fct_count() Calculates a count and prop table. starwars$sex %&gt;% factor() %&gt;% fct_count(sort = T, prop = T) ## # A tibble: 5 × 3 ## f n p ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 male 60 0.690 ## 2 female 16 0.184 ## 3 none 6 0.0690 ## 4 &lt;NA&gt; 4 0.0460 ## 5 hermaphroditic 1 0.0115 Second way with deplyr::count() Simply mutate a frequency and percentage column on a counted table. starwars %&gt;% count(sex) %&gt;% mutate(freq = n / sum(n)) %&gt;% mutate(perc = freq * 100) ## # A tibble: 5 × 4 ## sex n freq perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 16 0.184 18.4 ## 2 hermaphroditic 1 0.0115 1.15 ## 3 male 60 0.690 69.0 ## 4 none 6 0.0690 6.90 ## 5 &lt;NA&gt; 4 0.0460 4.60 13.3 Bar graph with count data Here is a situation where we calculated a count table for hair color - we summarized all values. If we then want to plot a bar graph based on that count table we run into problems, because ggplot2 is expecting a non-summarized or normal data frame. hair_color_table = starwars %&gt;% mutate(hair_color = fct_lump_min(hair_color, 2)) %&gt;% group_by(hair_color) %&gt;% summarise(n = n()) hair_color_table ## # A tibble: 7 × 2 ## hair_color n ## &lt;fct&gt; &lt;int&gt; ## 1 black 13 ## 2 blond 3 ## 3 brown 18 ## 4 none 37 ## 5 white 4 ## 6 Other 7 ## 7 &lt;NA&gt; 5 To tell the function that we have already summarized data, we add the argument stat = \"identity\" to the geom_bar() function. hair_color_table %&gt;% ggplot(aes(x = reorder(hair_color, n), y = n, fill = hair_color)) + geom_bar(stat = &quot;identity&quot;) + theme(legend.position = &quot;none&quot;) 13.4 Bar graph with percentage labels First we create a table with counts and percentages: d = starwars %&gt;% group_by(gender) %&gt;% summarise(count = n()) %&gt;% mutate(percentage = count/sum(count)) d ## # A tibble: 3 × 3 ## gender count percentage ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 feminine 17 0.195 ## 2 masculine 66 0.759 ## 3 &lt;NA&gt; 4 0.0460 Then we plot a graph with bar and with percentage labels. d %&gt;% ggplot(aes(gender, percentage, label = round(percentage, 2), fill = gender)) + geom_bar(stat = &quot;identity&quot;) + geom_label(aes(fill = NA), fill = &quot;white&quot;) + theme(legend.position = &quot;none&quot;) 13.5 Collapse factors to „Other” This syntax mutates the categorical variable homeworld into eight of its most frequent values. The other values are being collapsed into the categorical value „other”. starwars %&gt;% mutate(homeworld = fct_lump_n(homeworld, n = 8)) %&gt;% group_by(homeworld) %&gt;% summarise(mean(height, na.rm =T), mean(mass, na.rm = T), n()) ## # A tibble: 11 × 4 ## homeworld `mean(height, na.rm = T)` `mean(mass, na.rm = T)` `n()` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Alderaan 176. 64 3 ## 2 Corellia 175 78.5 2 ## 3 Coruscant 174. 50 3 ## 4 Kamino 208. 83.1 3 ## 5 Kashyyyk 231 124 2 ## 6 Mirial 168 53.1 2 ## 7 Naboo 175. 64.2 11 ## 8 Ryloth 179 55 2 ## 9 Tatooine 170. 85.4 10 ## 10 Other 173. 117. 39 ## 11 &lt;NA&gt; 139. 82 10 13.6 Filter for specific values We can easily filter out cases with certain column values, like for example the states of Hawai and Alaska. We use filter(), the operator ! and %in%. starwars %&gt;% filter(!homeworld%in%c(&quot;Tatooine&quot;,&quot;Naboo&quot;)) %&gt;% select(name, homeworld) ## # A tibble: 66 × 2 ## name homeworld ## &lt;chr&gt; &lt;chr&gt; ## 1 Leia Organa Alderaan ## 2 Obi-Wan Kenobi Stewjon ## 3 Wilhuff Tarkin Eriadu ## 4 Chewbacca Kashyyyk ## 5 Han Solo Corellia ## 6 Greedo Rodia ## 7 Jabba Desilijic Tiure Nal Hutta ## 8 Wedge Antilles Corellia ## 9 Jek Tono Porkins Bestine IV ## 10 Yoda &lt;NA&gt; ## # … with 56 more rows 13.7 Change bar colors in barplot You can manually pick the colors with fill and a vector containing the color values. Either in String, written out. starwars %&gt;% mutate(sex = fct_infreq(sex)) %&gt;% ggplot(aes(sex)) + geom_bar(fill = c(&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;black&quot;,&quot;grey&quot;)) Or with RGB Color Codes. starwars %&gt;% mutate(sex = fct_infreq(sex)) %&gt;% ggplot(aes(sex)) + geom_bar(fill = c(&quot;#003f5c&quot;,&quot;#58508d&quot;,&quot;#bc5090&quot;,&quot;#ff6361&quot;,&quot;#ffa600&quot;)) 13.8 Hide aes(color) mapping legend Here is an example where we want the bar colored based on the variable itself, but without the mapping legend. starwars %&gt;% mutate(sex = fct_infreq(sex)) %&gt;% ggplot(aes(sex, fill = sex)) + geom_bar() Hide the geom_bar legend. starwars %&gt;% mutate(sex = fct_infreq(sex)) %&gt;% ggplot(aes(sex, fill = sex)) + geom_bar(show.legend = F) Remove just the legend title: starwars %&gt;% mutate(sex = fct_infreq(sex)) %&gt;% ggplot(aes(sex, fill = sex)) + geom_bar() + theme(legend.title = element_blank()) Hide all legends created: starwars %&gt;% mutate(sex = fct_infreq(sex)) %&gt;% ggplot(aes(sex, fill = sex)) + geom_bar() + theme(legend.position = &quot;none&quot;) 13.9 Re-code values of categorical variables First way We can use fct_collapse()to create a new column with the new recoded values in it. Second way By using mutate, to create a new column with our own values and case_when, to run through our observations looking for defined cases, together with “variable” %in%, we can create our own groups. gapminder %&gt;% mutate(group = case_when( region %in% c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;,&quot;Southern Europe&quot;,&quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) ~ &quot;West&quot;, # If region is one of values -&gt; assign it &quot;West&quot; in new group column. region %in% c(&quot;Eastern Asia&quot;, &quot;South-Eastern Asia&quot;) ~ &quot;East Asia&quot;, region %in% c(&quot;Caribbean&quot;, &quot;Central America&quot;, &quot;South America&quot;) ~ &quot;Latin America&quot;, continent == &quot;Africa&quot; &amp; region != &quot;Northern Africa&quot; ~ &quot;Sub-Saharan&quot;, TRUE ~ &quot;Others&quot;)) %&gt;% # If nothing above applies -&gt; assign it &quot;Others&quot; in group column head(10) ## country year infant_mortality life_expectancy fertility ## 1 Albania 1960 115.40 62.87 6.19 ## 2 Algeria 1960 148.20 47.50 7.65 ## 3 Angola 1960 208.00 35.98 7.32 ## 4 Antigua and Barbuda 1960 NA 62.97 4.43 ## 5 Argentina 1960 59.87 65.39 3.11 ## 6 Armenia 1960 NA 66.86 4.55 ## 7 Aruba 1960 NA 65.66 4.82 ## 8 Australia 1960 20.30 70.87 3.45 ## 9 Austria 1960 37.30 68.75 2.70 ## 10 Azerbaijan 1960 NA 61.33 5.57 ## population gdp continent region group ## 1 1636054 NA Europe Southern Europe West ## 2 11124892 13828152297 Africa Northern Africa Others ## 3 5270844 NA Africa Middle Africa Sub-Saharan ## 4 54681 NA Americas Caribbean Latin America ## 5 20619075 108322326649 Americas South America Latin America ## 6 1867396 NA Asia Western Asia Others ## 7 54208 NA Americas Caribbean Latin America ## 8 10292328 96677859364 Oceania Australia and New Zealand West ## 9 7065525 52392699681 Europe Western Europe West ## 10 3897889 NA Asia Western Asia Others We turn this group variable into a factor to control the order of the levels: 13.10 Order color legend Order color legend by a variable’s values. 13.11 Show unique values Display all unique values of variable. distinct(starwars, species) # dplyr function ## # A tibble: 38 × 1 ## species ## &lt;chr&gt; ## 1 Human ## 2 Droid ## 3 Wookiee ## 4 Rodian ## 5 Hutt ## 6 Yoda&#39;s species ## 7 Trandoshan ## 8 Mon Calamari ## 9 Ewok ## 10 Sullustan ## # … with 28 more rows Note: distinct(dat$countries) doesn’t work. 13.12 Slice rows by maximum or minimum values Note: parameter n must be explicitly written, otherwise it throws an error. starwars %&gt;% slice_max(height, n = 5) ## # A tibble: 5 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Yarael … 264 NA none white yellow NA male mascul… ## 2 Tarfful 234 136 brown brown blue NA male mascul… ## 3 Lama Su 229 88 none grey black NA male mascul… ## 4 Chewbac… 228 112 brown unknown blue 200 male mascul… ## 5 Roos Ta… 224 82 none grey orange NA male mascul… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; Show me 5% of the lowest height rows. starwars %&gt;% slice_min(height, prop = 0.05) ## # A tibble: 4 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Yoda 66 17 white green brown 896 male mascu… ## 2 Ratts Ty… 79 15 none grey, blue unknown NA male mascu… ## 3 Wicket S… 88 20 brown brown brown 8 male mascu… ## 4 Dud Bolt 94 45 none blue, grey yellow NA male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; 13.13 Show Number of NAs For a quick check of how many missing values there are in a single column: sum(is.na(starwars$height)) ## [1] 6 And how many are not NAs. sum(!is.na(starwars$height)) ## [1] 81 For a more detailed overview of the whole dataset use skim(). It shows a very useful complete_rate which tells us how much of the column is disturbed by missing values. skimr::skim(starwars) Table 13.2: Data summary Name starwars Number of rows 87 Number of columns 14 _______________________ Column type frequency: character 8 list 3 numeric 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace name 0 1.00 3 21 0 87 0 hair_color 5 0.94 4 13 0 12 0 skin_color 0 1.00 3 19 0 31 0 eye_color 0 1.00 3 13 0 15 0 sex 4 0.95 4 14 0 4 0 gender 4 0.95 8 9 0 2 0 homeworld 10 0.89 4 14 0 48 0 species 4 0.95 3 14 0 37 0 Variable type: list skim_variable n_missing complete_rate n_unique min_length max_length films 0 1 24 1 7 vehicles 0 1 11 0 2 starships 0 1 17 0 5 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist height 6 0.93 174.36 34.77 66 167.0 180 191.0 264 ▁▁▇▅▁ mass 28 0.68 97.31 169.46 15 55.6 79 84.5 1358 ▇▁▁▁▁ birth_year 44 0.49 87.57 154.69 8 35.0 52 72.0 896 ▇▁▁▁▁ 13.14 Drop rows with missing values Drop rows that have NAvalues in a specific column, here in height. starwars %&gt;% drop_na(height) ## # A tibble: 81 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke S… 172 77 blond fair blue 19 male mascu… ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… ## 4 Darth … 202 136 none white yellow 41.9 male mascu… ## 5 Leia O… 150 49 brown light brown 19 fema… femin… ## 6 Owen L… 178 120 brown, grey light blue 52 male mascu… ## 7 Beru W… 165 75 brown light blue 47 fema… femin… ## 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… ## 9 Biggs … 183 84 black light brown 24 male mascu… ## 10 Obi-Wa… 182 77 auburn, wh… fair blue-gray 57 male mascu… ## # … with 71 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, ## # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; Drop all rows that contain NA in any column. starwars %&gt;% drop_na() ## # A tibble: 6 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Sk… 172 77 blond fair blue 19 male mascu… ## 2 Obi-Wan… 182 77 auburn, wh… fair blue-gray 57 male mascu… ## 3 Anakin … 188 84 blond fair blue 41.9 male mascu… ## 4 Chewbac… 228 112 brown unknown blue 200 male mascu… ## 5 Wedge A… 170 77 brown fair hazel 21 male mascu… ## 6 Darth M… 175 80 none red yellow 54 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; Filter out any NA containing rows. starwars %&gt;% na.exclude() ## # A tibble: 29 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke S… 172 77 blond fair blue 19 male mascu… ## 2 Darth … 202 136 none white yellow 41.9 male mascu… ## 3 Leia O… 150 49 brown light brown 19 fema… femin… ## 4 Owen L… 178 120 brown, grey light blue 52 male mascu… ## 5 Beru W… 165 75 brown light blue 47 fema… femin… ## 6 Biggs … 183 84 black light brown 24 male mascu… ## 7 Obi-Wa… 182 77 auburn, wh… fair blue-gray 57 male mascu… ## 8 Anakin… 188 84 blond fair blue 41.9 male mascu… ## 9 Chewba… 228 112 brown unknown blue 200 male mascu… ## 10 Han So… 180 80 brown fair brown 29 male mascu… ## # … with 19 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, ## # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; 13.15 Replace NAs Replace 0 with value you want as a replacement. data(na_example) sum(is.na(na_example)) ## [1] 145 no_nas &lt;- ifelse(is.na(na_example), 0, na_example) # &quot;if is NA is true, change value to 0, else keep the value (i.e. na_example)&quot; sum(is.na(no_nas)) ## [1] 0 13.16 The factor variable trap The FVT is about what happens when you try to return factorized vectors into numeric values. Let’s look at this with this code. z &lt;-factor(c(&quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;, &quot;12&quot;)) # We create an object by directly factorizing a vector. z ## [1] 12 13 14 15 12 ## Levels: 12 13 14 15 y &lt;- as.numeric(z) # Now we want to convert them into numeric values. y # What? ## [1] 1 2 3 4 1 This happened, because we picked up the on the factorization result. factor() assigns every element, based on its value, an integer number. typeof(z) # 1=12, 13=2, 14=3, 15=4, 12=1 ## [1] &quot;integer&quot; To fix this problem, first convert the object back to character and then to numeric. y &lt;- as.numeric(as.character(z)) y ## [1] 12 13 14 15 12 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
